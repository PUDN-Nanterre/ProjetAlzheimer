---
title: "Gestion de projet"
subtitle : "- Collaboration avec PUDN"
author: "Remaissa BENDIB, Hanjoon KO, Gracia YAN"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: true
    toc_float: true
    theme: readable
    df_print: paged
  pdf_document: default
---
* **M2 Information Communication, Données et Société
Université Paris Nanterre**


# Introduction

### Le projet

Dans le cadre du cours de Gestion de projet dispensé en M2 Information Communication, nous travaillons avec la PUDN (Plateforme Universitaire des données numériques). Ce projet, en partenariat avec la Fondation Deniker, porte sur la maladie d’Alzheimer. En analysant les données présentes sur HAL, nous établissons un état des lieux des contenus, du référencement existant des publications et proposons une méthodologie à destination des étudiants. Notre objectif est de montrer les possibilités de HAL mais aussi ses limites, en guidant les étapes d’extraction de données à partir d’une API jusqu’à la visualisation de ces données, tout en passant par le nettoyage, le traitement, l’analyse.

### Les outils utilisés

Dans cette analyse, nous mobiliserons plusieurs outils :

-   L’API de HAL\
    Pour récolter toutes les données nécessaires à notre projet.

-   RStudio\
    Pour traiter, analyser et visualiser ces données.

### Présentation HAL

Créée en 2001 à l’inititative du CNRS (Centre National de la Recherche Scientifique), HAL est une **plateforme numérique pluridisciplinaire** qui vise à centraliser des dépôts scientifiques (articles, thèses, rapports...) réalisés par des chercheurs afin d’en faciliter la diffusion, et le partage. La plateforme étant une **archive ouverte commune**, elle valorise la collaboration et le bien commun.

Les chercheurs peuvent publier eux-mêmes sur le site. Les contenus ne sont donc pas évalués en amont et sont archivés sur HAL que si une personne fait la démarche de publication.

C’est donc une immense **base de données**, alimentée par des chercheurs de toutes les disciplines, peu importe leur zone géographique, même si le portail n’est disponible qu’en français et en anglais.

### Extraction des données

**Qu'est-ce qu'une API ?**

*“Une API (application programming interface ou « interface de programmation d’application ») est une interface logicielle qui permet de « connecter » un logiciel ou un service à un autre logiciel ou service afin d’échanger des données et des fonctionnalités.”* (CNIL)

Pour résumer, l’API de HAL nous permet, entre autre, de connecter la base de données à notre interface de travail (pour notre cas RStudio). L’avantage d’une API est la mise à jour automatique de l’ensemble de la base de données dès qu’on le souhaite, sans avoir à faire une extraction manuelle avec les nouveaux contenus régulièrement.

Par exemple, notre base de données comprend l’ensemble des résultats de notre requête à partir du mot “Alzheimer”. Cependant, de nouvelles publications sont déposées sur HAL régulièrement. Il serait assez fastidieux et chronophage d’extraire tous ces résultats, manuellement, plusieurs fois par mois pour avoir un corpus complet et à jour.

L’API nous permet donc de mettre à jour cette base de données et de récolter les nouvelles publications récentes directement lorsqu’on travaille sur RStudio.

**Comment utiliser l’API de HAL ?**

⇒ Lien vers la méthodo faite par Abir Gabriel\
<https://gitlab.huma-num.fr/bchauvel/biblioalzheimer/-/blob/main/1_Extraction/FicheMethode_requeteHAL.Rmd?ref_type=heads>

**Extraction des données**

Pour mener à bien notre analyse, nous commençons par extraire des données pertinentes à partir de l'API HAL. Ce dernier est une ressource centralisée regroupant des publications scientifiques en libre accès.

Nous utilisons les requêtes suivantes pour obtenir les informations nécessaires :

```{r}

# Import des données dans R à partir de l'API HAL
url <- "https://api.archives-ouvertes.fr/search/hal/?q=alzheimer&rows=7000&wt=csv&indent=true&fl=docid,publicationDateY_i,docType_s,language_s,domain_s,primaryDomain_s,openAccess_bool,submitType_s,journalTitle_s,journalPublisher_s,authFullName_s,title_s,subTitle_s,citationRef_s,doiId_s,issue_s,journalIssn_s,volume_s,source_s,licence_s,files_s,journalTitleAbbr_s,title_st,submitType_s,type_s,page_s,publicationDate_s,keyword_s,en_keyword_s,fr_keyword_s,abstract_s,en_abstract_s,fr_abstract_s&sort=publicationDateY_i%20desc"

options(timeout=600) # pour forcer le temps limite de chargement (si faible connexion internet)
download.file(url, destfile = "AlzheimerHAL.csv")

url <- "https://api.archives-ouvertes.fr/search/hal/?q=alzheimer&rows=7000&wt=bibtex&indent=true&fl=docid,publicationDateY_i,docType_s,language_s,domain_s,primaryDomain_s,openAccess_bool,submitType_s,journalTitle_s,journalPublisher_s,authFullName_s,title_s,subTitle_s,citationRef_s,doiId_s,issue_s,journalIssn_s,volume_s,source_s,licence_s,files_s,journalTitleAbbr_s,title_st,submitType_s,type_s,page_s,publicationDate_s,keyword_s,en_keyword_s,fr_keyword_s,abstract_s,en_abstract_s,fr_abstract_s&sort=publicationDateY_i%20desc"

options(timeout=600) # pour forcer le temps limite de chargement (si faible connexion internet)
download.file(url, destfile = "AlzheimerHAL.bib")

# Le fichier csv est ensuite importé dans la session R sous forme de tableau de données
dataset_alzheimer <- read.csv("AlzheimerHAL.csv")
```

### Librairies

Le succès d'une analyse de données en R dépend souvent de l'utilisation des librairies appropriées. Les librairies fournissent des fonctions, des outils et des méthodes spécifiques qui facilitent la manipulation des données, la visualisation et l'analyse statistique. Dans ce guide, nous allons utiliser plusieurs librairies, parmi lesquelles, on retrouve :

#### a) Librairie permettant le chargement des données

-   **Readr :** Utilisée pour lire et charger des données tabulaires à partir de fichiers plats, offrant une méthode efficace pour **importer des données dans R.**

#### b) Librairies de nettoyage de données

-   **Labelled :** Utile pour **travailler avec des données étiquetées**.

-   **Stringr :** Offre des fonctionnalités avancées pour **la manipulation de chaînes de caractères,** particulièrement utile lors du nettoyage et de la préparation des données.

-   **Textcat** : Détecte la langue d'un document textuel. Elle est basée sur des modèles de langage statistiques pour déterminer la probabilité qu'un document soit écrit dans une langue spécifique.

#### c) Librairie de statistiques descriptives rapides

-   **Skimr :** Génère des statistiques descriptives rapides pour l'ensemble du jeu de données, offrant un **aperçu rapide des variables et de leur distribution.**

#### d) Librairies de manipulation et visualisation des données**

-   **Tidyverse :** Collection de librairies (ggplot2, dplyr, tidyr, etc.) pour **le nettoyage, la manipulation et la visualisation des données**. Adoptez une approche cohérente et "propre" de l'analyse des données.

-   **GGPlot2 :** Librairie de visualisation pour **créer une variété de graphiques et de diagrammes,** particulièrement utile pour représenter visuellement les tendances, les distributions et les relations dans les données.

-   **Ggrepel :** Extension de ggplot2 utilisée pour **éviter les chevauchements de texte dans les graphiques**, améliorant la lisibilité des étiquettes.

-   **KableExtra :** Permet de **créer des tableaux personnalisables** en formatant les sorties de dataframes, particulièrement utile lors de la présentation des résultats.

-   **Wordcloud et Wordcloud2 :** Utilisées pour **générer des nuages de mots basés** sur les titres des publications, offrant une visualisation des termes les plus fréquemment utilisés.

-   **RColorBrewer et Viridis :** Fournissent des **palettes de couleurs** pour les visualisations graphiques, améliorant la lisibilité et l'esthétique des graphiques.

#### e) Librairie de prétraitement du texte**

-   **TM (Text Mining) et SnowballC :** Utilisées pour le prétraitement du texte, notamment pour la création d'un corpus et le **nettoyage des données textuelles.**

    En garantissant que nous avons installé et chargé ces librairies au début de notre projet, nous nous assurons d'avoir toutes les fonctionnalités nécessaires pour effectuer une analyse complète.


```{r, include=FALSE}
# Chargement des librairies nécessaires

library(readr)      # Pour lire et charger des données tabulaires
library(labelled)   # Pour travailler avec des données étiquetées
library(questionr)  # Fournit des outils d'analyse de données en sciences sociales
library(stringr)    # Pour la manipulation avancée de chaînes de caractères
library(tidyverse)  # Collection de librairies (dplyr, ggplot2, tidyr) pour une analyse cohérente
library(skimr)      # Génère des statistiques descriptives rapides pour l'ensemble du jeu de données
library(ggplot2)    # Création de graphiques et de visualisations
library(ggrepel)    # Évite les chevauchements de texte dans les graphiques
library(kableExtra) # Personnalisation des tableaux
library(wordcloud)   # Nuage de mots
library(wordcloud2)  # Nuage de mots interactif
library(textcat) #  Classification automatique de texte 
```


# Nettoyage

Dans cette phase de nettoyage de données, nous débuterons en assignant des étiquettes à nos variables et à leurs modalités respectives. Ensuite, nous préparerons nos ensembles de données en vue de l'analyse, pour finalement les explorer en détail.

### 1. Étiquetage des variables

Les noms des variables du dataframe sont identiques aux champs sélectionnés lors de l'export depuis l'API. Pour une meilleure compréhension et lisibilité, nous ajoutons des étiquettes informatives à chaque variable en utilisant la fonction `var_label` du package `labelled`.

```{r, include=FALSE}
# Affichage des noms des variables
names(dataset_alzheimer)
```

Nous ajoutons des étiquettes aux noms de variables (avec le package `labelled`), pour les documenter et limiter les risques de mauvaises interprétations.

```{r, include=TRUE}
var_label(dataset_alzheimer) <- list(
  docid = "Identifiant HAL du dépôt", 
  publicationDateY_i = "Date de publication : année", 
  docType_s = "Type de document", 
  language_s = "Langue du document (code ISO 639-1 (alpha-2))", 
  domain_s = "Codes domaines du document", 
  primaryDomain_s = "Domaine primaire", 
  openAccess_bool = "publication en open access", 
  submitType_s = "Type de dépôt", 
  journalTitle_s = "Revue : Titre", 
  journalPublisher_s = "Revue : Editeur", 
  authFullName_s = "Auteur : Nom complet", 
  title_s = "Titres", 
  subTitle_s = "Sous-titre", 
  citationRef_s = "Citation abrégée", 
  doiId_s = "Identifiant DOI", 
  issue_s = "Numéro de revue", 
  journalIssn_s = "Revue : ISSN", 
  volume_s= "Volume", 
  source_s= "Source", 
  licence_s= "Droit d'auteur associé au document", 
  files_s= "URL des fichiers", 
  journalTitleAbbr_s= "Revue : Titre abrégé", 
  title_st= "Titres (sans les mots vides)", 
  type_s= "Type", page_s= "Pagination", 
  publicationDate_s= "Date de publication", 
  keyword_s = "Mots-clés", 
  en_keyword_s = "Mots-clés en anglais", 
  fr_keyword_s = "Mots-clés en français", 
  abstract_s = "Résumé", 
  en_abstract_s = "Résumé en anglais", 
  fr_abstract_s = "Résumé en français")
```

Des caractères ont été rajoutées à la fin de certains mots, après le séparateur  
- '_i' : interger (entier)  
- '_s', '_st' : string (chaîne de caractères)  
- '_bool' : boolean (booléen)

### 2. Étiquetage des modalités

#### **Étiquetage des types de documents** 

De la même manière, pour rendre les modalités plus explicites et compréhensibles, nous attribuons des libellés plus descriptifs pour les types de documents.

```{r, include=TRUE}
val_labels(dataset_alzheimer$docType_s) <- c(
  "Article dans une revue" = "ART", 
  "Article de blog scientifique" = "BLOG", 
  "Communication dans un congrès" = "COMM", 
  "Chapitre d'ouvrage" = "COUV", 
  "N°spécial de revue/special issue" = "ISSUE", 
  "Cours" = "LECTURE", 
  "Autre publication scientifique" = "OTHER", 
  "Ouvrages" = "OUV", 
  "Brevet" = "PATENT", 
  "Poster de conférence" = "POSTER", 
  "Rapport" = "REPORT", 
  "Thèse" = "THESE", 
  "Vidéo" = "VIDEO")
```

Ces étapes d’étiquetage des variables et des modalités **renforcent la compréhension du jeu de données et minimisent les risques de mauvaises** interprétations lors de l’analyse ultérieure.

#### **Étiquetage de la langue**

Nous renommons les langues pour avoir le mot en entier.

```{r, include=TRUE}
val_labels(dataset_alzheimer$language_s) <- c(
  "Allemand" = "de", 
  "Anglais" = "en", 
  "Espagnol" = "es", 
  "Français" = "fr", 
  "Portugais" = "pt", 
  "Ukrainien" = "uk")
```

#### **Étiquetage des domaines**  
Pour les domaines, nous leur attribuons des étiquettes et nous les agrégeons pour créer des catégories plus larges.

#### a) Agrégation des données liées aux domaines

Nous allons créer une variable `domaine_gpe`pour les regrouper les domaines.

Nous définissons une expression régulière (**`"^chim|^info|^math|^phys|^scco|^sde|^sdu|^sdv|^shs|^spi|^stat"`**) qui spécifie les motifs que nous cherchons. Chaque motif commence par le symbole **`^`** indiquant le début de la chaîne de caractères, suivi d'une abréviation de domaine. (Ex. “Informatique” remplace “info”). Ces motifs sont regroupés avec le symbole **`|`**, qui fonctionne comme un "OU". Ainsi, la recherche correspond à des chaînes qui commencent par l'une des abréviations spécifiées.

```{r, include=TRUE}
mots <- "^chim|^info|^math|^phys|^scco|^sde|^sdu|^sdv|^shs|^spi|^stat"
```

Cette partie du code prépare la création d’une variable `domaine_gpe` en extrayant les débuts de chaînes de caractères basés sur des critères spécifiques. Le but étant de regrouper les domaines en catégories plus larges en fonction de leurs abréviations.

Maintenant, nous allons créer la colonne `domaine_gpe` en question. Dans notre code, on spécifie que notre colonne contient les résultats de l'extraction des motifs spécifiés (`pattern = mots`) de la colonne existante `primaryDomain_s` (qui représente les domaines primaires).

```{r, include=TRUE}
dataset_alzheimer$domaine_gpe <- str_extract(dataset_alzheimer$primaryDomain_s, pattern = mots)
```

#### b) Attribution des étiquettes aux domaines

Nous allons ajouter des étiquettes aux modalités de la variable nouvellement créée, `domaine_gpe` :

```{r, include=TRUE}
val_labels(dataset_alzheimer$domaine_gpe) <- c( 
  "Chimie" = "chim", 
  "Informatique [cs]" = "info", 
  "Mathématiques [math]" = "math", 
  "Physique [physics]" = "phys", 
  "Économie et finance quantitative [q-fin]" = "qfin", 
  "Sciences cognitives" = "scco", 
  "Sciences de l'environnement" = "sde", 
  "Planète et Univers [physics]" = "sdu", 
  "Sciences du Vivant [q-bio]" = "sdv", 
  "Sciences de l'Homme et Société" = "shs", 
  "Sciences de l'ingénieur [physics]" = "spi", 
  "Statistiques [stat]" = "stat")
```

Les données et leur configuration sont sauvegardées dans le fichier "AlzheimerHAL.Rda" qui sera utilisé pour les analyses.

```{r, include=TRUE}
save(dataset_alzheimer, file = "AlzheimerHAL.Rda")
```

### 3. Préparation des datasets

Pour préparer les datasets que nous allons analyser et étudier, nous allons passer essentiellement par deux étapes :

-   [**Étape 1:**]{.underline} Préparation du dataset général

Dans cette étape, nous procéderons à la préparation du dataset général pour faire des visualisations sur les tendances générales.

-    [**Étape 2:**]{.underline} Préparation d’un dataset spécialisé pour les revues

Dans cette étape, nous créerons un sub-dataset spécifique qui se concentre exclusivement sur les publications de type “revues”. Cela permettra une analyse plus approfondie et ciblée sur ce sous-ensemble de données, facilitant ainsi l’extraction d’informations spécifiques liées aux revues.


-   [Étape 3]{.underline} : Enrichissement manuelle des données liées aux types de revues (cette étape viendra dans les analyses, car nous allons seulement enrichir le top20 des revues)


#### Étape 1 : Préparation d’un dataset général

A partir du dataset initial, nous allons créer un sous-ensemble de données que nous allons appeler dataset_shs_alzheimer.

Pour certaines de nos analyses, nous nous intéressons à l’ensemble des données afin de visualiser des tendances générales. Cependant, ces recherchent portent sur le domaine des Sciences Humaines et Sociales (SHS). Grâce à notre nettoyage précédent, nous pouvons chercher et sélectionner uniquement les sources catégorisées comme SHS ou “SSH” (en anglais) dans la variable de domaine.

```{r, include=TRUE}

dataset_shs_alzheimer <- dataset_alzheimer %>%
  filter(str_detect(tolower(domain_s), "shs|ssh") |
      str_detect(tolower(primaryDomain_s), "shs|ssh")
  )

# Utilisation de str_detect pour rechercher le motif "shs" ou "ssh" dans la colonne domain_s
# ou la colonne primaryDomain_s, avec la conversion en minuscules pour rendre la recherche insensible à la casse
```

Toujours dans l’optique de nettoyer nos données, vu que nous avons plusieurs types de publications, nous allons rassembler certaines avec d’autres.

Nous allons donc agréger nos données liées aux types de publications.

Les principales modifications incluent :

-   Regrouper "ISSUE" avec "ART" dans la catégorie "Article dans une revue".  
-   Regrouper "PROCEEDINGS" avec "COMM" dans la catégorie "Communication dans un congrès".  
-   Regrouper "HDR" avec "THESE".  
-   Regrouper "UNDEFINED" avec d'autres catégories d'"Autre publication scientifique".  
-   Regrouper "COUV" avec "Ouvrages".

```{r, include=TRUE}
dataset_shs_alzheimer <- dataset_shs_alzheimer %>%
  mutate(docType_s = case_when(
    docType_s %in% c("ART", "ISSUE") ~ "Article dans une revue",
    docType_s %in% c("BLOG") ~ "Article de blog scientifique",
    docType_s %in% c("COMM", "PROCEEDINGS") ~ "Communication dans un congrès",
    docType_s %in% c("LECTURE") ~ "Cours",
    docType_s %in% c("OTHER", "VIDEO", "TRAD", "UNDEFINED") ~ "Autre publication scientifique",
    docType_s %in% c("OUV", "COUV") ~ "Ouvrages",
    docType_s %in% c("PATENT") ~ "Brevet",
    docType_s %in% c("POSTER") ~ "Poster de conférence",
    docType_s %in% c("REPORT") ~ "Rapport",
    docType_s %in% c("THESE", "HDR") ~ "Thèse",
    TRUE ~ as.character(docType_s)  
  ))

```


#### Étape 2 : Préparation d’un dataset spécialisé pour les revues

**REVUES**  
L’API nous donnant toutes les informations des colonnes sélectionnées, nous avons des contenus qui ne nous intéresse pas pour notre recherche. Afin de se concentrer seulement sur les informations relatives aux revues SHS, nous créons un sous dataset.

Grâce à notre agrégation précédente, nous avons toutes les données des articles de revues réunies.

```{r, include=TRUE}
unique_doctype <- unique(dataset_shs_alzheimer$docType_s) 
print(unique_doctype)
```

Nous allons tout d'abord filtrer le jeu de données shs pour inclure uniquement les lignes où docType_s est "Article dans une revue" et où la colonne journalTitle_s contient les mots "Revue", "revue", "review" ou "Review" car il se peut que quelques revues ne soient pas indiquées comme étant des revues dans le type du document (undefined)

```{r, include=TRUE}
revues_shs_alzheimer <- dataset_shs_alzheimer %>%
  filter(
    docType_s %in% c("Article dans une revue") |
      str_detect(tolower(journalTitle_s), "revue|review")
  )
```

Nous avons 394 publications de revues.

**LANGUES**

Les publications sont répertoriées dans différentes langues selon la langue du texte détectée. Cependant, de nombreuses erreurs apparaissent. Pour pallier ces confusions et pouvoir établir une répartition plus juste des contenus, nous devons comparer si la détection de la langue correspond à ce que nous indique la colonne de la langue. Si non, nous nettoyons à la main. Nous allons uniquement effectuer le nettoyage de la langue ici, car notre analyse linguistique est spécifiquement axée sur les revues.

Pour nettoyer nos données de langue, nous allons tout d'abord les afficher pour avoir une vue sur l'existant :

```{r, include=TRUE}

# Créer une table des occurrences de chaque langue
language_counts <- table(revues_shs_alzheimer$language_s)

# Afficher la table
print(language_counts)

```

- *TEXTCAT*  
Pour nettoyer notre colonne de langue, nous allons utiliser la librarie textcat. Cette librairie basée sur des modèles NLP pré-entrainés permet la détection automatique de la langue d'un texte. Il s'agit plus précisemment d'un N_gramm de la base de donnés pour 26 langues basé sur le corpus miltilingue de l'initiative Corpus Européen. Pour plus d'informations, visitez : (<https://cran.r-project.org/web/packages/textcat/textcat.pdf>)

Notre approche consiste à détecter la langue du document à partir du titre, puis à la comparer avec la langue déjà saisie sur HAL (dans la colonne "language_s"). Nous procéderons ensuite à une vérification manuelle des données incohérentes, c'est-à-dire lorsque le résultat de la détection diffère de la langue enregistrée sur HAL. Cette vérification sera effectuée en consultant la publication en ligne pour assurer la précision des données.

```{r, include=TRUE}

# Utilisation de la fonction textcat pour détecter la langue des titres d'articles
revues_shs_alzheimer$language_verification <- sapply(revues_shs_alzheimer$title_s, textcat)

# Afficher les 20 premières lignes du DataFrame avec la nouvelle colonne language_verification
head(revues_shs_alzheimer, 20)

```

Nous allons créer une nouvelle colonne appelée 'language_verification'. Cependant, la bibliothèque renvoie le résultat en tant que 'french' pour le français et 'english' pour l'anglais. Étant donné que nous voulons comparer nos résultats avec ceux déjà saisis sur HAL, nous devons ajuster la sortie :

```{r, include=FALSE}

#Modifier les valeurs en "fr" ou "en" en fonction de la langue détectée
revues_shs_alzheimer$language_verification <- ifelse(revues_shs_alzheimer$language_verification == "french", "fr",
                                                     ifelse(revues_shs_alzheimer$language_verification == "english", "en", NA))

```

Nous allons comparer ces données à celles déjà saisies sur HAL (donc language_s)

```{r, include=TRUE}

#Filtrer les lignes où language_verification n'est pas égal à language_s
lignes_langue_différente <- revues_shs_alzheimer %>%
  mutate(language_verification = as.character(language_verification),
         language_s = as.character(language_s)) %>%
  filter(language_verification != language_s)

```

```{r, include=FALSE}

#Afficher les lignes
print(lignes_langue_différente)
```

Maintenant que nous avons les lignes qui ne correspondent pas, nous allons vérifier manuellement si c'est le cas ou non, puis nous allons corriger les incohérences.

Nous allons corriger les données incorrectes à partir de l'identifiant docid :

Il est à noter que nous avons choisi 'docid' car c'est l'identifiant unique, et donc il ne se reproduit pas dans chaque ligne du jeu de données. Il renvoie à chaque publication."

```{r, include=TRUE}

revues_shs_alzheimer <- revues_shs_alzheimer %>%
  mutate(language_s = case_when(
    docid %in% c(1806639, 1240621, 984498, 1240619, 4375787, 3355729, 4457542, 4380879, 3524754,
                 4047932, 4391680, 2440803, 4201490, 1584518, 3817981, 4330838, 3730267, 4331180,
                 4375866, 3730264, 3497539, 3730263, 4334832, 3726046, 3643277, 4226480, 4334932,
                 4375647) ~ "fr",
    TRUE ~ language_s
  ))

```

Nous rencontrons un obstacle pour les publications incluant  

- un titre en 2 langues différentes (ex: titre français mais aussi écrit en agnalis à la suite)
- un titre qui contient des mots de langue étrangère au reste du titre (ex : le terme “care” utilisé dans un titre français)

**ÉDITEURS**  
Enfin, un nettoyage est nécéssaire sur les éditeurs. En effet, l’éditeur “Elsevier”  apparaît sous 2 formes : “Elsevier” et “Elsevier Masson”. Nous les regroupons sous un même terme. Nous allons donc mettre à jour la colonne ‘journalPublisher_s’

Pour ce faire, nous allons utiliser la fonction **`mutate`** du package **`dplyr`** pour effectuer une transformation sur la colonne **`journalPublisher_s`** du dataframe **`revues_shs_alzheimer`**. La transformation est réalisée avec la fonction **`if_else`**, qui permet de remplacer les occurrences de "Elsevier" dans la colonne **`journalPublisher_s`** par "Elsevier Masson".

En d'autres termes, chaque fois que la valeur de la colonne **`journalPublisher_s`** est "Elsevier", elle est mise à jour avec la valeur "Elsevier Masson".

```{r, include=TRUE}

revues_shs_alzheimer <- revues_shs_alzheimer %>%   
  mutate(     journalPublisher_s = if_else(journalPublisher_s == "Elsevier", "Elsevier Masson", journalPublisher_s)   )
```

Nous allons remplir les lignes vides dans journalPublisher_s par des NA pour pouvoir retirer les NA lors de nos analyses :

```{r}
revues_shs_alzheimer$journalPublisher_s <- replace(revues_shs_alzheimer$journalPublisher_s, revues_shs_alzheimer$journalPublisher_s == "", NA)

# Créer et afficher la table des occurrences
table_journal_publisher <- table(revues_shs_alzheimer$journalPublisher_s)
#print(table_journal_publisher)
```

# Exploration post-nettoyage

Dans cette section dédiée à l'analyse des fréquences, nous allons explorer nos données nettoyées. L'objectif de cette étape est de mieux comprendre la distribution des valeurs dans certaines variables clés. En créant des tableaux de fréquence, nous pourrons identifier les tendances, les modalités les plus courantes, et détecter d'éventuelles anomalies.

Pour afficher les fréquences, nous allons utiliser la fonction **`freq()`**qui permet d’obtenir les paramètres descriptifs des variables.

### Fréquence des types de publications

```{r, include=TRUE}

# Affichage des fréquences des types de documents
freq(dataset_alzheimer$docType_s, sort = "dec", valid = FALSE, total = TRUE) %>% knitr::kable(caption = "Types de documents présents dans le corpus")
```

### Fréquence de la langue

```{r, include=TRUE}
# Affichage des fréquences des langues dans le corpus
freq(dataset_alzheimer$language_s, sort = "dec", valid = FALSE, total = TRUE) %>% 
  knitr::kable(caption = "Langue du document")
```

### Fréquence des domaines

```{r, include=TRUE}
freq(dataset_alzheimer$primaryDomain_s, sort = "dec", valid = FALSE, total = TRUE) %>% 
  head(20) %>% 
  knitr::kable(caption = "Domaines primaires détaillés (les 20 plus fréquents)")
```


# Analyses et Interprétations des résultats

### 1. Les tendances annuelles de publications

Pour avoir une idée globale de la recherche et de la popularité de HAL, il est intéressant d’explorer l’évolution temporelle des publications sur le sujet de la maladie d’Alzheimer dans le domaine qui nous intéresse : les Sciences Humaines et Sociales.

Nous allons tout d'abord calculer le nombre de publications pour chaque année et type de document en créant un dataframe de résumé.

```{r, include=TRUE}

# Résumé des tendances de publication par année et type de document, en excluant les types "UNDEFINED".
tendances_publication <- dataset_shs_alzheimer %>%
  group_by(publicationDateY_i, docType_s) %>%
  summarise(count = n()) %>%
  filter(!(docType_s %in% c("UNDEFINED")))
view(tendances_publication)

```

Visualisation des données avec la palette de couleurs Dark2 :

```{r, include=TRUE}

# Création d'un graphique à barres empilées montrant les tendances des publications SHS sur la maladie d'Alzheimer depuis 1998.
ggplot(tendances_publication, aes(x = publicationDateY_i, y = count, fill = docType_s)) +
  geom_bar(stat = "identity", position = "stack", width = 0.7, color = "black") +  # Barres empilées
  scale_fill_brewer(palette = "Dark2") +  # Palette de couleurs
  labs(
    title = "Tendances des publications SHS sur la maladie d’Alzheimer depuis 1998",
    x = "Année de publication",
    y = "Nombre de publications",
    fill = "Type de document"
  ) +  # Titres et légendes
  theme_minimal() +  # Style minimal
  scale_x_continuous(breaks = unique(tendances_publication$publicationDateY_i), labels = unique(tendances_publication$publicationDateY_i)) +  # Ajustements de l'axe x
  theme(axis.text.x = element_text(angle = 45, hjust = 1, size=7))  # Ajustements des étiquettes de l'axe x

```

La première publication accessible sur HAL portant sur notre sujet est un ouvrage qui a été publié en 1998. Cette date, antérieure à celle de la création de HAL, montre que la référence de publication indiquée peut aller au-delà des limites de la plateforme. HAL peut donc être un espace d’archivage de recherches scientifiques pour les ressources de plus de 25 ans.

Entre 2001 et 2006, il y a peu de publications. Mais il y a systématiquement un ou plusieurs articles publiés (à l’exception de 2004).

Il y a un pic de publications en 2012. Cette date marque la fin du Plan Alzheimer 2008-2012, projet politique qui tente d’améliorer la recherche française sur la maladie d’Alzheimer. On peut penser que de nombreuses recherches se sont intéressées, à cette période, à la maladie. Les résultats ont ensuite été publiés sur HAL.

2017 est une année où HAL enregistre le plus grand nombre de publications sur son site dans cette discipline.

Les articles de revues sont globalement le type de document qui y est le plus déposé.

### 2. La répartition des publications SHS (maj)

Pour visualiser toutes ces publications sous un autre angle, il est possible de regrouper ces données pour avoir une image résumant la répartition totale de ces publications en SHS sans l’aspect temporel. Pour cela, l’analyse sera représentée sous forme de camembert et gagne en lisibilité grâce aux couleurs et aux étiquettes de pourcentage.

Avant de créer le camambert, nous allons tout d'abord créer un résumé des comptes et pourcentages pour chaque catégorie docType_s :

```{r, include=TRUE}

# Calcul du nombre d'occurrences de chaque type de document et calcule le pourcentage par rapport au total.
count_data <- dataset_shs_alzheimer %>%
  count(docType_s) %>%  # Calcul du nombre d'occurrences par type de document
  mutate(percentage = prop.table(n) * 100)  # Ajout d'une colonne avec le pourcentage par rapport au total

```

Maintenant, nous allons créer un camembert avec des étiquettes de pourcentage à l'extérieur et des flèches :

```{r, include=TRUE}

# Utilisation de ggplot2 et ggrepel pour générer un graphique polarisé avec des étiquettes représentant les pourcentages.
ggplot(count_data, aes(x = "", y = n, fill = docType_s)) +
  geom_bar(width = 1, stat = "identity", color = "white") +  

  geom_label_repel(aes(
    label = scales::percent(percentage / 100),
    fill = docType_s
  ),
  position = position_stack(vjust = 0.5),
  box.padding = 0.5,
  point.padding = 0.3,
  arrow = arrow(length = unit(0.02, 'npc')),
  show.legend = FALSE) +  # Ajout des étiquettes avec les pourcentages

  coord_polar("y", start = 0) +  
  scale_fill_brewer(palette = "Dark2") +  # Définir la palette de couleurs
  theme_void() +  # Suppression des éléments du thème par défaut
  theme(axis.text = element_blank()) +  
  labs(title = "Répartition des types de publications sur la maladie d'Alzheimer",
       fill = "Type de publication")  # Ajout des légendes

```

Les barres sont remplies avec la couleur **`#E6AB02`** qui fait partie de la palette Dark2.

-   La catégorie la plus prédominante est "Article dans une revue", représentant environ 47.24% de l'ensemble des types de documents.

-   "Communication dans un congrès" représente une part significative, soit environ 19.78% de l'ensemble.

-   Les "Ouvrages" représentent environ 14.51% de l'ensemble, contribuant de manière notable.

-   Environ 10.19% des types de documents sont classés comme "Thèse", une part significative mais inférieure à d'autres catégories.

-   Les "Poster de conférence" constituent une part relativement faible, soit environ 2.99%.

-   La catégorie "Rapport" représente environ 2.28% de l'ensemble, une part modérée.

-   Environ 2.99% des types de documents sont classés comme "Autre publication scientifique".

    En d'autres termes, le grand nombre d’articles de revues est plus parlant dans ce graphique en comparaison au précédent. Ici, on peut affirmer que presque 50% des publications sur l’Alzheimer, en SHS, disponibles sur HAL, sont des articles de revues.

    Cependant, ce pourcentage est le résultat des dépôts sur HAL. Il n’est pas représentatif de la recherche globale dans ce domaine puisqu’il ne s’agit que d’une seule base de données pour la recherche scientifique. Il serait intéressant de compléter ces recherches et de les comparer avec des données sur d’autres sites comme Cairn.

    En deuxième position, on voit que les communications dans des congrès sont davantage publiés que les ouvrages. On peut supposer que le dépôt sur HAL, une plateforme ouverte, n’incite pas les auteurs à soumettre leurs livres qu’il est possible d’acheter.

### 3. Top des revues les plus actives

Pour analyser les revues les plus actives en SHS, c'est-à-dire celles qui publient le plus de publications, couvrant les domaines principaux ou secondaires, ou ayant publié des publications SHS, nous avons trié, dans l'ordre croissant, 20 revues :

```{r, include=TRUE}

# Analyse des revues les plus actives
top_revues <- revues_shs_alzheimer %>%
  count(journalTitle_s) %>%  # Calcul du nombre de publications par revue
  arrange(desc(n)) %>%  # Tri par ordre décroissant du nombre de publications
  head(20)  # Sélection des 20 revues les plus actives
```

Comme déjà mentionné lors de la préparation du mini-dataset des revues, nous allons enrichir nos données. Pour ce faire, nous allons associer à chaque revue un domaine principal en cherchant via le moteur de recherche HAL, dans quels domaines la revue publiait le plus.

Voici comment nous allons procéder, étape par étape :

**1.Recherche du nom de la revue sur le moteur HAL :** On va utiliser le nom de la revue, par exemple, "Retraite et société", pour effectuer une recherche avancée sur le moteur HAL en spécifiant qu'il s'agit d'une revue.

**2. Filtration** **par sujet (Subject field) :** Une fois que nous avons les résultats de la recherche, à savoir les documents publiés par cette revue sur HAL, nous allons utiliser le ruban de filtration à gauche de la page de résultats. Nous allons rechercher la section "Subject field" dans le ruban de filtrage.

**3.** **Identification du domaine principal :** Dans la section "Subject field", nous allons examiner la fréquence de publications de la revue dans chaque domaine pour identifier le domaine dans lequel la revue publie le plus fréquemment.

```{r, include=TRUE}

# Enrichissement des données manuellement via HAL
top_revues <- top_revues %>%
  mutate(domaine_revue = case_when(
    journalTitle_s == "Journal of Alzheimer's Disease" ~ "Sciences du Vivant",
    journalTitle_s == "NPG: Neurologie - Psychiatrie - Gériatrie" ~ "SHS",
    journalTitle_s == "Gériatrie et psychologie & neuropsychiatrie du vieillissement" ~ "Sciences du Vivant",
    journalTitle_s == "Psychologie & NeuroPsychiatrie du vieillissement" ~ "Sciences du Vivant",
    journalTitle_s == "Retraite et société" ~ "SHS",
    journalTitle_s == "Current Alzheimer Research" ~ "Sciences du Vivant",
    journalTitle_s == "Revue Neurologique" ~ "Sciences du Vivant",
    journalTitle_s == "Gérontologie et Société" ~ "SHS",
    journalTitle_s == "L'Encéphale" ~ "Sciences du Vivant",
    journalTitle_s == "Cortex" ~ "Sciences du Vivant",
    journalTitle_s == "Neuropsychology" ~ "Sciences cognitives",
    journalTitle_s == "Soins Gérontologie" ~ "Sciences du Vivant",
    journalTitle_s == "Annales Médico-Psychologiques, Revue Psychiatrique" ~ "SHS",
    journalTitle_s == "Archives of Clinical Neuropsychology" ~ "Sciences cognitives",
    journalTitle_s == "Brain and Cognition" ~ "Sciences du Vivant",
    journalTitle_s == "Journal de droit de la santé et de l'assurance maladie" ~ "SHS",
    journalTitle_s == "Journal of Clinical and Experimental Neuropsychology" ~ "Sciences cognitives",
    journalTitle_s == "L'Évolution Psychiatrique" ~ "SHS",
    journalTitle_s == "RDSS. Revue de droit sanitaire et social" ~ "SHS",
    journalTitle_s == "Sciences Sociales et Santé" ~ "SHS",
    TRUE ~ "Autre"
  ))

```

Nous allons ajouté une nouvelle colonne appelée **`Abbréviation_revues`** au dataframe **`top_revues`**. Chaque abréviation est attribuée à une revue spécifique. Cela permet identification rapide des revues sans avoir à utiliser les noms complets de chaque revue.

**Voici l'abbréviation des titres de revues que nous allons utiliser :**\
- J Alzheimers Dis : Journal of Alzheimer's Disease\
- NPG : NPG: Neurologie - Psychiatrie - Gériatrie\
- Gériatr. psychol. Neuropsychiatr. vieil. : Gériatrie et psychologie & neuropsychiatrie du vieillissement\
- Psychol Neuropsychiatr Vieil : Psychologie & NeuroPsychiatrie du vieillissement\
- Retraite et société\
- Current Alzheimer Research\
- Rev Neurol (Paris) : Revue Neurologique\
- GES : Gérontologie et Société\
- Encephale : L'Encéphale\
- Cortex\
- Neuropsychology\
- Soins Gerontol : Soins Gérontologie\
- Annales Médico-Psychologiques : Annales Médico-Psychologiques, Revue Psychiatrique\
- Arch Clin Neuropsychol : Archives of Clinical Neuropsychology\
- Brain Cogn : Brain and Cognitio\
- JDSAM : Journal de droit de la santé et de l'assurance maladie\
- JCEN = Journal of Clinical and Experimental Neuropsychology\
- L'Évolution Psychiatrique\
- RDSS : RDSS. Revue de droit sanitaire et social\
- Sciences Sociales et Santé

```{r, include=TRUE}

# Ajout des abbréviations aux noms des revues 
top_revues$Abbréviation_revues <- c("J Alzheimers Dis","NPG","Gériatr. psychol. Neuropsychiatr. vieil.","Psychol Neuropsychiatr Vieil","Retraite et société","Current Alzheimer Research","Rev Neurol (Paris)","GES","Encephale","Cortex","Neuropsychology","Soins Gerontol","Annales Médico-Psychologiques","Arch Clin Neuropsychol","Brain Cogn","JDSAM","JCEN","L'Évolution Psychiatrique","RDSS","Sciences Sociales et Santé")
```

Maintenant nous allons créer notre graphique des revues les plus actives qui mentionne également le domaine des revues :

```{r, include=TRUE}

# Création d'un graphique à barres des revues les plus actives avec domaine_revue
ggplot(top_revues, aes(x = n, y = fct_reorder(Abbréviation_revues, n), fill = domaine_revue)) +
  geom_bar(stat = "identity", color = "black") +
  labs(
    title = "Revues les plus actives",
    x = "Nombre de publications",
    y = "Revue",
    fill = "Domaine de revue"
  ) +
  scale_fill_brewer(palette = "Dark2") +
  scale_y_discrete(labels = function(x) str_wrap(x, width = 28)) +
  theme_minimal()

```

Cette méthode d'enrichissemnt de données manuelle nous a permis de segmenter les domaines mais le manque d’information fournie au préalable sur la spécialité de ces revues pose un problème.

En effet, cette catégorisation par nos soins ne représente pas la réalité de ces revues.

Par exemple, le Journal de droit de la santé et de l’assurance maladie est très présent sur HAL dans le domaine SHS alors qu’il s’agit d’une revue qui porte davantage sur le droit et la médecine. Pour pallier ces erreurs, les auteurs doivent publier davantage sur HAL pour essayer d’atteindre une représentativité plus juste.

En ce qui concerne notre graphe, d’après les 20 revues les plus actives, les trois domaines majeurs qui traitent du sujet de l’Alzheimer sur HAL sont les sciences cognitives, les sciences du vivant et les sciences humaines et sociales.

En s’intéressant aux noms de ces revues, on remarque une récurrence de ce qui relève du domaine de la psychologie.

### 4. Les auteurs les plus actifs (maj)

Nous allons analyser les auteurs les plus actifs dans les revues SHS. Le but étant d'identifier les auteurs avec le plus grand nombre de publications sur Alzheimer.

Nous allons sélectionner les colonnes essentielles pour faciliter notre analyse :

Il s'agit de : (docid, publicationDateY_i, docType_s, language_s, domain_s, primaryDomain_s, openAccess_bool, submitType_s, journalTitle_s, journalPublisher_s, authFullName_s, title_s, subTitle_s, citationRef_s, publicationDate_s).

```{r, include=TRUE}
revues_shs_auteur <- revues_shs_alzheimer %>% select(1:14,26) 
```

Nous allons distinguer les auteurs de chaque publication.

```{r, include=TRUE}
revues_shs_auteur$AutList <- strsplit(revues_shs_auteur$authFullName_s, ",")
```

Nous allons créer une autre colonne pour calculer le nombre d'auteurs de chaque publication.

```{r, include=TRUE}
revues_shs_auteur$nbAut <- NA

for(i in 1:length(revues_shs_auteur$AutList)){
  revues_shs_auteur$nbAut[[i]] <- length(revues_shs_auteur$AutList[[i]])}
```

Nous allons dupliquer chaque lignes de dataframe le nombre de fois spécifié dans la colonne 'nbAut' et attribuer des valeurs d'ordre. Si la valeur 'docid' de la ligne actuelle est identique à celle de la ligne précédente, alors la valeur d'ordre de la ligne actuelle est définie comme étant égale à la valeur d'ordre de la ligne précédente augmentée de 1. Sinon, la valeur d'ordre de la ligne actuelle est définie comme étant 1.

```{r, include=TRUE}
revues_shs_auteur <- revues_shs_auteur[rep(1:nrow(revues_shs_auteur), 
                                           revues_shs_auteur$nbAut),]

revues_shs_auteur$ordre <- NA

revues_shs_auteur$ordre[1] <- 1

for(i in 2:length(revues_shs_auteur$ordre)){
  {if(revues_shs_auteur$docid[i]==revues_shs_auteur$docid[i-1]){ revues_shs_auteur$ordre[i] <- revues_shs_auteur$ordre[i-1]+1 } else (revues_shs_auteur$ordre[i] <- 1)}
}
```

Nous allons extraire la valeur correspondante à la position de la colonne ordre dans la colonne 'AutList' de chaque ligne et l'assigner à la colonne 'Auteur'.

```{r, include=TRUE}
revues_shs_auteur$Auteur <- NA

for(i in 1:length(revues_shs_auteur$ordre)){
  revues_shs_auteur$Auteur[i] <- revues_shs_auteur$AutList[[i]][revues_shs_auteur$ordre[i]]}
```

Nous allons créer un tableau pour voir le nom des auteurs et leur nombre de publications. Après avoir réarrangé en ordre de sa fréquentation, on sélectionne les dix auteurs les plus actifs.

```{r, include=TRUE}
aut_actif <- table(revues_shs_auteur$Auteur)
aut_actif_df <- data.frame(aut = names(aut_actif), frequentation = as.numeric(aut_actif)) 
top_aut <- aut_actif_df[order(aut_actif_df$frequentation, decreasing = TRUE), ]
top_10 <- head(top_aut, 10)
```

Afin de comprendre les disciplines principales des dix auteurs, nous recherchons sur Internet leurs informations professionnelles et les ajouter dans une nouvelle colonne.

```{r, include=TRUE}
aut_discipline <- c("Psychologie", "Psychologie", "Gérontologie", "Gérontologie", "Gérontologie", "Gérontologie", "Psychologie", "Philosophie", "Psychologie", "Psychologie")

top_10$Discipline <- factor(aut_discipline, levels = c("Psychologie", "Gérontologie", "Philosophie"))
```

```{r, include=TRUE}
ggplot(top_10, aes(x = reorder(aut, -frequentation), y = frequentation, fill = Discipline)) +
  geom_bar(stat = "identity") + 
  scale_fill_manual(values = c("Psychologie" = "#1B9E77", "Gérontologie" = "#D95F02", "Philosophie" = "#7570B3")) +
  labs(title = "Les auteurs les plus actifs", x = "Auteur", y = "Nombre de publications") + 
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

Mohamad El Haj, dans le domaine de psychologie, est l'auteur le plus actif avec 38 publications sur la maladie d'alzheimer. Entre les dix auteurs, cinq personnes contribuent au domaine de psychologie, quatre au gérontologie et un à la philosophie.

Nous filtrons les cinq meilleurs auteurs et montrons les revues dans lesquelles ils ont écrit leurs recherches.

```{r, include=FALSE}
library(kableExtra)
```

```{r, include=TRUE}
aut_rev <- revues_shs_auteur %>% 
  filter(Auteur == "Mohamad El Haj" | 
           Auteur == "Philippe Allain" |
           Auteur == "Cédric Annweiler" |
           Auteur == "Jean-Pierre Jacus" |
           Auteur == "Karim Gallouj")

aut_rev <- revues_shs_auteur %>%
  filter(Auteur %in% c("Mohamad El Haj", "Philippe Allain", "Cédric Annweiler", "Jean-Pierre Jacus", "Karim Gallouj")) %>%
  distinct(Auteur, journalTitle_s) %>%
  group_by(Auteur) %>%
  summarize(Journals = paste(journalTitle_s, collapse = ", "))

```

```{r, include=TRUE}
kbl(aut_rev, caption = "Dans quelle revue les auteurs actifs ont-ils publié ?") %>%
  kable_styling(full_width = FALSE) %>%
  row_spec(0, bold = TRUE, color = "white", background = "skyblue")
```

On a établi le top 10 des auteurs les plus actifs en leur ajoutant une discipline majeure d’après leurs informations présentes sur d’autres sites Internet.

HAL ne donne pas directement les informations relatives aux champs disciplinaires des auteurs. Cette recherche doit se faire manuellement et peut être assez contraignante si on cherche à représenter plus de 20 auteurs. De plus, un auteur peut avoir plusieurs disciplines majeures, ce qui est difficilement représentable sur le graphique.

La psychologie représente la discipline principale de la moitié de ces auteurs. Cela semble assez vraisemblable puisqu’on a remarqué précédemment que les revues en rapport avec la psychologie sont très présentes dans nos recherches. Cette discipline se place avant la gérontologie (l’étude du vieillissement), ce qui semble assez logique étant donné la maladie dont il est question.

Il y a tout de même un aspect philosophique à travers un auteur ce qui montre la diversité, voire la complémentarité, des angles d’approche pour étudier la maladie d’Alzheimer.

### 5. La répartition des langues des publications (maj)

Dans cette partie, nous explorons la diversité linguistique des publications dans le domaine des sciences humaines et sociales (SHS) liées à la maladie d'Alzheimer. L'analyse vise à comprendre la répartition des langues utilisées dans les revues académiques traitant de ce sujet spécifique.

La première étape consiste à créer une table des fréquences des langues présentes dans notre ensemble de données.

```{r, include=TRUE}

# Créer une table des langues
langue_pays_freq <- revues_shs_alzheimer %>%
  count(language_s) %>%
  arrange(desc(n))
view(revues_shs_alzheimer)
```

On crée la visualisation :

```{r, include=TRUE}

# Créer la visualisation avec un graphique à barres empilées
ggplot(langue_pays_freq, aes(x = fct_reorder(language_s, n), y = n)) +  # Utilisation de ggplot avec les données et l'esthétique des axes
  geom_bar(stat = "identity", fill = "#E6AB02", color = "black") +  # Ajout des barres empilées avec des couleurs spécifiées
  labs(
    title = "Répartition des langues des publications SHS sur la maladie d'Alzheimer",  # Ajout d'un titre au graphique
    x = "Langue",  # Nom de l'axe des x
    y = "Nombre de publications",  # Nom de l'axe des y
  ) +
  geom_text(aes(label = n), vjust = -0.5, color = "black", size = 3, fontface="bold") +  # Ajout des étiquettes de données au-dessus des barres
  theme_minimal() +  # Utilisation d'un thème minimal pour le graphique
  theme(axis.text.x = element_text(hjust = 1))  # Ajustement de l'emplacement du texte de l'axe des x

```

A partir des textes des publications au sein des revues portant sur la thématique de l’Alzheimer, on peut distinguer les différentes langues.

Alors que le nettoyage réalisé plus haut sur les langues nous montrait leur petite diversité (plus de 5 sur l’ensemble des données), celle-ci se réduit drastiquement lorsqu’il s’agit de publications dans une revue. Les articles sont en anglais principalement, puis en français. Il n’y en a qu’un en portugais.

On peut penser que HAL est davantage populaire dans la recherche francophone et anglophone. Tout en sachant que HAL ne permet pas de faire ces segmentations facilement et de manière pratique. En effet, il y a beaucoup d’erreurs de catégorisation. Des ressources en français se retrouvaient dans la partie anglaise et inversement.

Il a fallut corriger ces erreurs manuellement.

### 6. Les mots récurrents dans les titres de revus SHS (maj)

Nous allons créer un nuage de mots basé sur les titres des publications pour visualiser les termes les plus fréquents. Les mots "Alzheimer", "maladie" et "disease" sont exclus pour l'analyse.

```{r, include=FALSE}
library(wordcloud) 
library(wordcloud2)
library(tm)
library(SnowballC) 
library(RColorBrewer)
library(viridis)
library(grid)
library(gridExtra)
```

**Les publications françaises**

Après avoir créé un corpus pour analyser le texte, nous mettons les caractères en minuscules et enlevons les nombres, les stopwords, les trois mois basiques et les espaces vides.

```{r, include=TRUE, warning=FALSE}
revues_shs_alzheimer_fr <- revues_shs_alzheimer %>% filter(language_s == "fr")
nuage_corpus_fr <- Corpus(VectorSource(revues_shs_alzheimer_fr$title_s))

nuage_corpus_clean_fr<-tm_map(nuage_corpus_fr,tolower)
nuage_corpus_clean_fr<-tm_map(nuage_corpus_clean_fr,removeNumbers)
nuage_corpus_clean_fr<-tm_map(nuage_corpus_clean_fr,removeWords,stopwords("english"))
nuage_corpus_clean_fr<-tm_map(nuage_corpus_clean_fr,removeWords,stopwords("fr"))
nuage_corpus_clean_fr<-tm_map(nuage_corpus_clean_fr,removeWords, c("alzheimer", "maladie", "disease"))
nuage_corpus_clean_fr <- tm_map(nuage_corpus_clean_fr, content_transformer(function(x) gsub("[[:punct:]]", "", x)))
nuage_corpus_clean_fr<-tm_map(nuage_corpus_clean_fr,removePunctuation)
nuage_corpus_clean_fr<-tm_map(nuage_corpus_clean_fr,stripWhitespace)
```

Le mot 'personnes' est le plus couramment utilisé dans les titres des publications françaises. Cependant, étant donné que ce mot est neutre, il est préférable de faire référence aux mots en violet les plus utilisés en deuxième place, tels que 'clinique', 'cognitive', 'mémoire', etc. Il existe également des mots anglais tels que 'mild' et 'care', mais c'est parce que les auteurs ont directement utilisé ces termes anglais dans les titres en français.

*Exemple : "Conscience des troubles dans la maladie d’Alzheimer et le mild cognitive impairment"*

```{r, include=TRUE, warning=FALSE}
wordcloud(nuage_corpus_clean_fr, max.words = 50, min.freq = 1,
          colors = brewer.pal(8, "Dark2"), rot.per = 0.35)
grid.text("Les mots-clés français les plus utilisés", x = 0.5, y = 0.98, 
          gp = gpar(fontsize = 15, fontface = "bold"))

```

**Les publications anglaises**

```{r, include=TRUE, warning=FALSE}
revues_shs_alzheimer_en <- revues_shs_alzheimer %>% filter(language_s == "en")

nuage_corpus_en <- Corpus(VectorSource(revues_shs_alzheimer_en$title_s))
nuage_corpus_clean_en<-tm_map(nuage_corpus_en,tolower)
nuage_corpus_clean_en<-tm_map(nuage_corpus_clean_en,removeNumbers)
nuage_corpus_clean_en<-tm_map(nuage_corpus_clean_en,removeWords,stopwords("english"))
nuage_corpus_clean_en<-tm_map(nuage_corpus_clean_en,removeWords,stopwords("fr"))
nuage_corpus_clean_en<-tm_map(nuage_corpus_clean_en,removeWords, c("alzheimer", "maladie", "disease"))
nuage_corpus_clean_en <- tm_map(nuage_corpus_clean_en, content_transformer(function(x) gsub("[[:punct:]]", "", x)))
nuage_corpus_clean_en<-tm_map(nuage_corpus_clean_en,removePunctuation)
nuage_corpus_clean_en<-tm_map(nuage_corpus_clean_en,stripWhitespace)

```

Nous pouvons observer plusieurs mots-clés importants qui représentent les sujets souvent recherchés dans les publications sur la maladie d'Alzheimer. Les mots les plus fréquemment utilisés sont 'memory' et 'cognitive', suivis de 'patients', 'impairment', 'mild', 'dementia', etc.

```{r, include=TRUE, warning=FALSE}
wordcloud(nuage_corpus_clean_en,max.words = 50, min.freq =1, colors = brewer.pal(8, "Dark2"), rot.per=0.35)
grid.text("Les mots-clés anglais les plus utilisés", x = 0.5, y = 0.98, 
          gp = gpar(fontsize = 15, fontface = "bold"))
```

En résumé, en approfondissant cet aspect linguistique, on a généré 2 nuages de mots à partir des termes utilisés dans les titres des publications.

Il a été décidé de séparer les titres anglais de ceux français pour pouvoir comparer les 2. En excluant les mots “Alzheimer”, “maladie” et “disease”, ces deux graphiques nous montrent les termes les plus fréquents.

Pour obtenir ce résultat lisible et interprétable, un nettoyage était nécessaire pour enlever les nombres, la ponctuation, les stopwords (les “mots vides” comme les articles, conjonctions, etc...). On remarque qu’il reste toutefois certains mots qui ne sont pas toujours pertinents : “quand”, “chez”.

Ce nettoyage des mots est assez fastidieux et demande une intervention en partie manuelle.

Dans le nuage de mot français, on remarque que le terme “mild” est toujours présent. Pendant nos analyses, on avait remarqué que les anglicismes dans les titres français n’étaient pas détectés et apparaissent donc dans notre nuage de mots français. Il est pertinent de les garder puisqu’ils sont utilisés en français.

D’autres mots anglais pouvaient apparaître malgré le filtrage de la langue, dû au fait que certains auteurs mettent leur titre en français et en anglais.

Dans ce cas, il est nécessaire d’avoir recourt à un nettoyage manuel, parfois au cas par cas.

En comparant ces deux graphiques, on peut voir que du côté francophone, le terme “personnes” est au même niveau que “cognitive” et “memory”. Ceci peut montrer que la recherche francophone s’intéresse avant tout à l’individu ou aux personnes aidant les malades. Tandis que la recherche anglaise explore les phénomènes relatifs au cerveau.

Les termes équivalents en français : “cognitive” et “mémoire” n’apparaissent qu’en 4ème position sur le nuage de mots. Avant cela, les mots “troubles” et “aidants” sont davantage récurrents. Puis, en rose, des mots comme “diagnostic”, “prise”, “clinique”, “éthique” laissent penser que la recherche francophone se concentre davantage sur l’accompagnement et la gestion des personnes atteintes de la maladie d’Alzheimer. Du côté anglophone, les termes qui ressortent sont relatifs à l’aspect concret de la maladie, ce qui se passe de manière technique et scientifique chez les personnes concernées par cette maladie.

### 7. Les institutions les plus actives (maj)

Pour identifier les institutions les plus actives dans la publication d'articles dans le domaine des sciences humaines et sociales (SHS) concernant la maladie d'Alzheimer, nous allons analyser les données de nos revues.

Nous allons supprimer les valeurs manquantes (NA) dans la colonne **`journalPublisher_s`** à cette étape plutôt que lors du nettoyage ultérieur. En procédant ainsi, nous nous assurons de conserver les lignes nécessaires à d'autres analyses tout en éliminant les données manquantes spécifiquement dans le contexte de la colonne des éditeurs (**`journalPublisher_s`**)

```{r, include=TRUE}
revues_shs_alzheimer <- revues_shs_alzheimer %>%
  filter(!is.na(journalPublisher_s))
```

Cette ligne de code utilise la fonction **`filter`** pour exclure les lignes où la colonne **`journalPublisher_s`** a des valeurs **`NA`**. Cela nettoie le dataframe en retirant les observations avec des valeurs manquantes dans la colonne spécifiée.

Nous allons maintenant sélectionner le top 10 des institutions de revue les plus actifs:

```{r, include=FALSE}
top_institutions <- revues_shs_alzheimer %>%
  count(journalPublisher_s) %>%
  arrange(desc(n)) %>%
  head(10)
```

Cette séquence de commandes utilise **`count`** pour compter le nombre d'occurrences de chaque éditeur de revue, puis **`arrange`** pour les trier par ordre décroissant en fonction du nombre de publications (**`n`**). Enfin, **`head(10)`** est utilisé pour sélectionner les 10 éditeurs avec le nb le plus élevé.

Nous allons maintenant créer une visualisation :

```{r, include=TRUE}
ggplot(top_institutions, aes(x = n, y = fct_reorder(journalPublisher_s, n))) +
  geom_bar(stat = "identity", fill = "#E6AB02", color = "black") +
  labs(
    title = "Institutions les plus actives dans les revues SHS",
    x = "Nombre de publications",
    y = "Institution"
  ) +
  scale_x_continuous(breaks = seq(0, max(top_institutions$n), by = 7)) +
  scale_y_discrete(labels = function(x) str_wrap(x, width = 40))+
  theme_minimal()
```

L’institution la plus active sur le sujet montre deux fois plus de publications avec celle en deuxième position. Il s’agit d’une grande maison d’édition installée depuis plusieurs siècles. On peut imaginer que les financements sont plus conséquents que parmi d’autres institutions ce qui lui permet d’être très présente dans le domaine.

# Conclusion

Ce travail à partir de l’API de HAL nous a permis de voir la richesse d’informations qu’il est possible de mobiliser. Toutes ces données sont précieuses pour mener à bien des projets de recherches et permettent d’approfondir de nombreux sujets.

Cependant, dans toutes les étapes de notre projet, nous avons été confrontés aux difficultés de lisbilité des données. En effet, des nettoyages sur plusieurs variables et à différents moments ont été plus que nécessaires pour bien comprendre les informations et pouvoir les rendre tout aussi compréhensibles dans la restitution de nos analyses. Une grande partie a été nettoyée à la main, ce qui est chronophage et fastidieux.

Selon les projets, il est plus ou moins possible d’utiliser l’API sans avoir recours à un nettoyage profond, mais dans d’autres cas, il est impératif de passer par là pour obtenir des données propres et exploitables. Cela limite l’accessibilité des données pourtant très nombreuses dans cette immense base de données.

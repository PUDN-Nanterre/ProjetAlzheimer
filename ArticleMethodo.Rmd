---
title: "Gestion de projet (collaboration avec PUDN)"
author: "Remaissa BENDIB, Hanjoon KO, Gracia YAN"
date: "`r Sys.Date()`"
output:
  html_document:
    df_print: paged
  pdf_document: default
---

# Introduction
Ce projet du cours de Gestion de projet de la formation en Information et Communication à l'Univeristé Paris Nanterre a pour objectif de collaborer avec des organisations françaises pour un projet spécifique. Notre projet, en collaboration avec PUDN (Plateforme Universitaire de Données de Nanterre), vise à gérer les recherches sur la maladie d'Alzheimer archivées sur HAL. En utilisant l'API de HAL, nous téléchargeons les données des articles et les nettoyons pour analyser les recherches sur la maladie ainsi que les revues dans lesquelles elles sont publiées.

# Présentation HAL

# Extraction à partir de l'API

Reprendre la méthodo de Abir Gabriel en la modifiant ?

<https://gitlab.huma-num.fr/bchauvel/biblioalzheimer/-/blob/main/1_Extraction/FicheMethode_requeteHAL.Rmd?ref_type=heads>

```{r, include=FALSE}
library(readr)
library(labelled)
library(questionr)
library(stringr)
library(tidyverse)
library(skimr)
```

```{r}

# Import des données dans R à partir de l'API HAL
url <- "https://api.archives-ouvertes.fr/search/hal/?q=alzheimer&rows=7000&wt=csv&indent=true&fl=docid,publicationDateY_i,docType_s,language_s,domain_s,primaryDomain_s,openAccess_bool,submitType_s,journalTitle_s,journalPublisher_s,authFullName_s,title_s,subTitle_s,citationRef_s,doiId_s,issue_s,journalIssn_s,volume_s,source_s,licence_s,files_s,journalTitleAbbr_s,title_st,submitType_s,type_s,page_s,publicationDate_s,keyword_s,en_keyword_s,fr_keyword_s,abstract_s,en_abstract_s,fr_abstract_s&sort=publicationDateY_i%20desc"

options(timeout=600) # pour forcer le temps limite de chargement (si faible connexion internet)
download.file(url, destfile = "AlzheimerHAL.csv")

url <- "https://api.archives-ouvertes.fr/search/hal/?q=alzheimer&rows=7000&wt=bibtex&indent=true&fl=docid,publicationDateY_i,docType_s,language_s,domain_s,primaryDomain_s,openAccess_bool,submitType_s,journalTitle_s,journalPublisher_s,authFullName_s,title_s,subTitle_s,citationRef_s,doiId_s,issue_s,journalIssn_s,volume_s,source_s,licence_s,files_s,journalTitleAbbr_s,title_st,submitType_s,type_s,page_s,publicationDate_s,keyword_s,en_keyword_s,fr_keyword_s,abstract_s,en_abstract_s,fr_abstract_s&sort=publicationDateY_i%20desc"

options(timeout=600) # pour forcer le temps limite de chargement (si faible connexion internet)
download.file(url, destfile = "AlzheimerHAL.bib")

# Le fichier csv est ensuite importé dans la session R sous forme de tableau de données (le fichier bibtext sera utilisé ultérieurement).
dataset_alzheimer <- read.csv("AlzheimerHAL.csv")
```

# Nettoyage des données

## Étiquetage des variables

Les variables portent les mêmes noms que les champs sélectionnés dans le lien d'export dans l'API : noms des variables du dataframe

```{r, include=TRUE}
names(dataset_alzheimer)
```

On ajoute des étiquettes aux noms de variables (avec le package labelled), pour les documenter et limiter les risques de mauvaises interprétations.

```{r, include=TRUE}
var_label(dataset_alzheimer) <- list(
  docid = "Identifiant HAL du dépôt", 
  publicationDateY_i = "Date de publication : année", 
  docType_s = "Type de document", 
  language_s = "Langue du document (code ISO 639-1 (alpha-2))", 
  domain_s = "Codes domaines du document", 
  primaryDomain_s = "Domaine primaire", 
  openAccess_bool = "publication en open access", 
  submitType_s = "Type de dépôt", 
  journalTitle_s = "Revue : Titre", 
  journalPublisher_s = "Revue : Editeur", 
  authFullName_s = "Auteur : Nom complet", 
  title_s = "Titres", 
  subTitle_s = "Sous-titre", 
  citationRef_s = "Citation abrégée", 
  doiId_s = "Identifiant DOI", 
  issue_s = "Numéro de revue", 
  journalIssn_s = "Revue : ISSN", 
  volume_s= "Volume", 
  source_s= "Source", 
  licence_s= "Droit d'auteur associé au document", 
  files_s= "URL des fichiers", 
  journalTitleAbbr_s= "Revue : Titre abrégé", 
  title_st= "Titres (sans les mots vides)", 
  type_s= "Type", page_s= "Pagination", 
  publicationDate_s= "Date de publication", 
  keyword_s = "Mots-clés", 
  en_keyword_s = "Mots-clés en anglais", 
  fr_keyword_s = "Mots-clés en français", 
  abstract_s = "Résumé", 
  en_abstract_s = "Résumé en anglais", 
  fr_abstract_s = "Résumé en français")
```

## Étiquetage des modalités

De même pour rendre les modalités plus explicites, par exemple, le type et la langue du document :

```{r, include=TRUE}
val_labels(dataset_alzheimer$docType_s) <- c(
  "Article dans une revue" = "ART", 
  "Article de blog scientifique" = "BLOG", 
  "Communication dans un congrès" = "COMM", 
  "Chapitre d'ouvrage" = "COUV", 
  "N°spécial de revue/special issue" = "ISSUE", 
  "Cours" = "LECTURE", 
  "Autre publication scientifique" = "OTHER", 
  "Ouvrages" = "OUV", 
  "Brevet" = "PATENT", 
  "Poster de conférence" = "POSTER", 
  "Rapport" = "REPORT", 
  "Thèse" = "THESE", 
  "Vidéo" = "VIDEO")
```

```{r, include=TRUE}

freq(dataset_alzheimer$docType_s, sort = "dec", valid = FALSE, total = TRUE) %>% knitr::kable(caption = "Types de documents présents dans le corpus")

```

## Langue

```{r, include=TRUE}
val_labels(dataset_alzheimer$language_s) <- c(
  "Allemand" = "de", 
  "Anglais" = "en", 
  "Espagnol" = "es", 
  "Français" = "fr", 
  "Portugais" = "pt", 
  "Ukrainien" = "uk")
```

```{r, include=TRUE}
freq(dataset_alzheimer$language_s, sort = "dec", valid = FALSE, total = TRUE) %>% 
  knitr::kable(caption = "Langue du document")
```

Les codes domaines sont particulièrement détaillés:

```{r, include=TRUE}
freq(dataset_alzheimer$primaryDomain_s, sort = "dec", valid = FALSE, total = TRUE) %>% 
  head(20) %>% 
  knitr::kable(caption = "Domaines primaires détaillés (les 20 plus fréquents)")
```

On créé une variable `domaine_gpe`pour les regrouper.

extraire les débuts de chaînes de caractères dans une nouvelle variable

termes recherchés : commence par (\^) chim ou (\|) commence par,....

```{r, include=TRUE}
mots <- "^chim|^info|^math|^phys|^scco|^sde|^sdu|^sdv|^shs|^spi|^stat"
```

domaine_gpe prend les modalités extraites de primaryDomains

```{r, include=TRUE}
dataset_alzheimer$domaine_gpe <- str_extract(dataset_alzheimer$primaryDomain_s, pattern = mots)
```

ajoute des étiquettes aux modalités de domaine_gpe

```{r, include=TRUE}
val_labels(dataset_alzheimer$domaine_gpe) <- c( 
  "Chimie" = "chim", 
  "Informatique [cs]" = "info", 
  "Mathématiques [math]" = "math", 
  "Physique [physics]" = "phys", 
  "Économie et finance quantitative [q-fin]" = "qfin", 
  "Sciences cognitives" = "scco", 
  "Sciences de l'environnement" = "sde", 
  "Planète et Univers [physics]" = "sdu", 
  "Sciences du Vivant [q-bio]" = "sdv", 
  "Sciences de l'Homme et Société" = "shs", 
  "Sciences de l'ingénieur [physics]" = "spi", 
  "Statistiques [stat]" = "stat")
```

## Tableau de fréquence

```{r, include=TRUE}
freq(dataset_alzheimer$domaine_gpe, sort = "dec", valid = FALSE) %>% 
  knitr::kable(caption = "Domaines primaires regroupés")

```

# modifier
TODO On crée aussi une variable domaine_shs, pour identifier les articles dont au moins un des domaines est shs.

```{r, include=TRUE}
dataset_alzheimer$language_gpe <- as.character(dataset_alzheimer$language_s)

dataset_alzheimer$language_gpe[dataset_alzheimer$language_s != "en" & dataset_alzheimer$language_s != "fr"] <- "autre"

val_labels(dataset_alzheimer$language_gpe) <- c( "Anglais" = "en", "Français" = "fr", "Autres" = "autre" )

freq(dataset_alzheimer$language_gpe) %>% 
  knitr::kable(caption = "Langues du corpus")
```

Sauvegarde des données et de leurs étiquettes

On sauvegarde les données et leur configuration dans le fichier "AlzheimerHAL.Rda" qui sera utilisé pour faire les analyses.

```{r, include=TRUE}
save(dataset_alzheimer, file = "AlzheimerHAL.Rda")
```

## Préparation des dataset avant de faire des analyses

-   [Étape 1 :]{.underline} Préparation du dataset général

Dans cette étape, nous procéderons à la préparation du dataset général pour faire des visualisations sur les tendances générales.

-   [Étape 2 :]{.underline} Création d'un sub-dataset spécialisé pour les revues

Dans cette étape, nous créerons un sub-dataset spécifique qui se concentre exclusivement sur les publications de type "revues". Cela permettra une analyse plus approfondie et ciblée sur ce sous-ensemble de données, facilitant ainsi l'extraction d'informations spécifiques liées aux revues.

-   [Etape 3]{.underline} : Enrichissement manuelle des données liées aux types de revues (cette étape viendra dans les analyses, car nous allons seulement enrichir le top20 des revues)

### Étape 1

Préparation du dataset général :

A partir du dataset initial, nous allons créer un sous-ensemble de données que nous allons appeler dataset_shs_alzheimer.

Notre filtrage des lignes est basé sur des conditions spécifiques : la fonction str_detect nous permettra de rechercher le motif "shs" ou "ssh" dans la colonne domain_s ou la colonne primaryDomain_s, avec la conversion en minuscules pour rendre la recherche insensible à la casse.

```{r, include=TRUE}

dataset_shs_alzheimer <- dataset_alzheimer %>%
  
  # Filtrage des lignes basé sur des conditions spécifiques
  filter(
    # Utilisation de str_detect pour rechercher le motif "shs" ou "ssh" dans la colonne domain_s
    # ou la colonne primaryDomain_s, avec la conversion en minuscules pour rendre la recherche insensible à la casse
    str_detect(tolower(domain_s), "shs|ssh") |
      str_detect(tolower(primaryDomain_s), "shs|ssh")
  )

```

Toujours dans l'optique de nettoyer nos données, vu que nous avons plusieurs types de publications, nous allons rassembler certaines avec d'autres.

Nous allons donc agréger nos données liées aux types de publications.

Les principales modifications incluent :

-   Regrouper "ISSUE" avec "ART" dans la catégorie "Article dans une revue".

-   Regrouper "PROCEEDINGS" avec "COMM" dans la catégorie "Communication dans un congrès".

-   Regrouper "HDR" avec "THESE".

-   Regrouper "UNDEFINED" avec d'autres catégories d'"Autre publication scientifique".

-   Regrouper "COUV" avec "Ouvrages".

```{r, include=TRUE}

# Agréger les données dans la colonne docType_s
dataset_shs_alzheimer <- dataset_shs_alzheimer %>%
  mutate(docType_s = case_when(
    docType_s %in% c("ART", "ISSUE") ~ "Article dans une revue",
    docType_s %in% c("BLOG") ~ "Article de blog scientifique",
    docType_s %in% c("COMM", "PROCEEDINGS") ~ "Communication dans un congrès",
    docType_s %in% c("LECTURE") ~ "Cours",
    docType_s %in% c("OTHER", "VIDEO", "TRAD", "UNDEFINED") ~ "Autre publication scientifique",
    docType_s %in% c("OUV", "COUV") ~ "Ouvrages",
    docType_s %in% c("PATENT") ~ "Brevet",
    docType_s %in% c("POSTER") ~ "Poster de conférence",
    docType_s %in% c("REPORT") ~ "Rapport",
    docType_s %in% c("THESE", "HDR") ~ "Thèse",
    TRUE ~ as.character(docType_s)  
  ))

```

### Étape 2

Création d'un dataset pour analyser les données liées aux revues

Affichage des données liées aux types de documents

```{r, include=TRUE}

unique_doctype <- unique(dataset_shs_alzheimer$docType_s) 
print(unique_doctype)
```

Vu que nous souhaitons nous concentrer sur les revues SHS, nous allons donc créer un nouveau dataset qui prend seulement les données liées aux revues.

Nous allons tout d'abord filtrer le jeu de données shs pour inclure uniquement les lignes où docType_s est "Article dans une revue" et où la colonne journalTitle_s contient les mots "Revue", "revue", "review" ou "Review" car il se peut que quelques revues ne soient pas indiquées comme étant des revues dans le type du document (undefined)


```{r, include=TRUE}
revues_shs_alzheimer <- dataset_shs_alzheimer %>%
  filter(
    docType_s %in% c("Article dans une revue") |
      str_detect(tolower(journalTitle_s), "revue|review")
  )
```

Nous avons donc 394 publications de revues.

**Nettoyage des données liées à la langue :**

Pour nettoyer nos données de langue, nous allons tout d'abord les afficher :

```{r, include=TRUE}

# Créer une table des occurrences de chaque langue
language_counts <- table(revues_shs_alzheimer$language_s)

# Afficher la table
print(language_counts)


```

```{r, include=TRUE}
library(textcat)
```
Pour nettoyer notre colonne de langue, nous allons utiliser la librarie textcat. Cette librairie basé sur des modèles NLP pré-entrainés permet la détection automatique de la langue d'un texte. Il s'agit plus précisemment d'un N_gramm de la base de donnés pour 26 langues basé sur le corpus miltilingue de l'initiative Corpus Européen (<https://cran.r-project.org/web/packages/textcat/textcat.pdf>)

```{r, include=FALSE}
revues_shs_alzheimer$language_verification <- sapply(revues_shs_alzheimer$title_s, textcat)

# Afficher le dataframe avec la colonne language_verification
head(revues_shs_alzheimer, 20)

```

Nous avons choisi de détecter la langue de la colonne title_s

Une colonne language_verification a été donc créé. Sauf que la libraire affiche le résultat comme étant : soit : french pour français, soit english pour anglais. Vu que nous souhaitons comparer nos résultats à ceux déjà saisis sur HAL, nous devons modifier l'output :

```{r}

# Modifier les valeurs en "fr" ou "en" en fonction de la langue détectée
revues_shs_alzheimer$language_verification <- ifelse(revues_shs_alzheimer$language_verification == "french", "fr",
                                                     ifelse(revues_shs_alzheimer$language_verification == "english", "en", NA))

```

Nous allons comparer ces données à celles déjà saisies sur HAL (donc language_s)

Filtrer les lignes où language_verification n'est pas égal à language_s

```{r, include=TRUE}

lignes_langue_différente <- revues_shs_alzheimer %>%
  mutate(language_verification = as.character(language_verification),
         language_s = as.character(language_s)) %>%
  filter(language_verification != language_s)

```

Afficher les lignes

```{r, include=FALSE}
print(lignes_langue_différente)
```

Maintenant que nous avons les lignes qui ne correspondent pas, nous allons vérifier manuellement si c'est le cas ou non, puis nous allons corriger les incohérences.

Nettoyer les données liées à la langue à partir de l'identifiant docid

```{r, include=TRUE}

revues_shs_alzheimer <- revues_shs_alzheimer %>%
  mutate(language_s = case_when(
    docid %in% c(1806639, 1240621, 984498, 1240619, 4375787, 3355729, 4457542, 4380879, 3524754,
                 4047932, 4391680, 2440803, 4201490, 1584518, 3817981, 4330838, 3730267, 4331180,
                 4375866, 3730264, 3497539, 3730263, 4334832, 3726046, 3643277, 4226480, 4334932,
                 4375647) ~ "fr",
    TRUE ~ language_s
  ))

```

Il y a également une incohérence dans les lignes concernant les éditeurs, nous allons mettre à jour la colonne journalPublisher_s :

```{r, include=TRUE}
revues_shs_alzheimer <- revues_shs_alzheimer %>%   
  mutate(     journalPublisher_s = if_else(journalPublisher_s == "Elsevier", "Elsevier Masson", journalPublisher_s)   )
```

Nous allons remplir les lignes vides dans journalPublisher_s par des NA pour pouvoir retirer les NA lors de nos analyses :

```{r}
revues_shs_alzheimer$journalPublisher_s <- replace(revues_shs_alzheimer$journalPublisher_s, revues_shs_alzheimer$journalPublisher_s == "", NA)

# Créer et afficher la table des occurrences
table_journal_publisher <- table(revues_shs_alzheimer$journalPublisher_s)
#print(table_journal_publisher)
```

# Analyse pour répondre à nos questions

### 1. Top des revues les plus actives

Nous allons tout d'abord ressortir le top 20 des revues ayant le nb le plus élevé des publications

```{r}
# Analyse des revues les plus actives
top_revues <- revues_shs_alzheimer %>%
  count(journalTitle_s) %>%  # Compter le nombre de publications par revue
  arrange(desc(n)) %>%  # Trier par ordre décroissant du nombre de publications
  head(20)  # Sélectionner les 20 revues les plus actives
```

Nous allons attribuer une discipline à chacune de ces revues. Pour ce faire, nous avons chercher sur le moteur de recherche HAL pour identifier le domaine dans lequel chaque revue publie le plus.

```{r}
top_revues <- top_revues %>%
  mutate(domaine_revue = case_when(
    journalTitle_s == "Journal of Alzheimer's Disease" ~ "Sciences du Vivant",
    journalTitle_s == "NPG: Neurologie - Psychiatrie - Gériatrie" ~ "SHS",
    journalTitle_s == "Gériatrie et psychologie & neuropsychiatrie du vieillissement" ~ "Sciences du Vivant",
    journalTitle_s == "Psychologie & NeuroPsychiatrie du vieillissement" ~ "Sciences du Vivant",
    journalTitle_s == "Retraite et société" ~ "SHS",
    journalTitle_s == "Current Alzheimer Research" ~ "Sciences du Vivant",
    journalTitle_s == "Revue Neurologique" ~ "Sciences du Vivant",
    journalTitle_s == "Gérontologie et Société" ~ "SHS",
    journalTitle_s == "L'Encéphale" ~ "Sciences du Vivant",
    journalTitle_s == "Cortex" ~ "Sciences du Vivant",
    journalTitle_s == "Neuropsychology" ~ "Sciences cognitives",
    journalTitle_s == "Soins Gérontologie" ~ "Sciences du Vivant",
    journalTitle_s == "Annales Médico-Psychologiques, Revue Psychiatrique" ~ "SHS",
    journalTitle_s == "Archives of Clinical Neuropsychology" ~ "Sciences cognitives",
    journalTitle_s == "Brain and Cognition" ~ "Sciences du Vivant",
    journalTitle_s == "Journal de droit de la santé et de l'assurance maladie" ~ "SHS",
    journalTitle_s == "Journal of Clinical and Experimental Neuropsychology" ~ "Sciences cognitives",
    journalTitle_s == "L'Évolution Psychiatrique" ~ "SHS",
    journalTitle_s == "RDSS. Revue de droit sanitaire et social" ~ "SHS",
    journalTitle_s == "Sciences Sociales et Santé" ~ "SHS",
    TRUE ~ "Autre"
  ))

```

```{r}
top_revues$Abbréviation <- c("J Alzheimers Dis","NPG","Gériatr. psychol. Neuropsychiatr. vieil.","Psychol Neuropsychiatr Vieil","Retraite et société","Current Alzheimer Research","Rev Neurol (Paris)","GES","Encephale","Cortex","Neuropsychology","Soins Gerontol","Annales Médico-Psychologiques","Arch Clin Neuropsychol","Brain Cogn","JDSAM","JCEN","L'Évolution Psychiatrique","RDSS","Sciences Sociales et Santé")
```

Maintenant nous allons créer notre graphique des revues les plus actives qui mentionne également le domaine des revues

```{r}

# Créer un graphique à barres des revues les plus actives avec domaine_revue
ggplot(top_revues, aes(x = n, y = fct_reorder(Abbréviation, n), fill = domaine_revue)) +
  geom_bar(stat = "identity", color = "black") +
  labs(
    title = "Revues les plus actives",
    x = "Nombre de publications",
    y = "Revue",
    fill = "Domaine de revue"
  ) +
  scale_fill_brewer(palette = "Dark2") +
  scale_y_discrete(labels = function(x) str_wrap(x, width = 28)) +
  theme_minimal()

```

  
  **Abbréviation des titres de revues**  
  - J Alzheimers Dis : Journal of Alzheimer's Disease  
  - NPG : NPG: Neurologie - Psychiatrie - Gériatrie  
  - Gériatr. psychol. Neuropsychiatr. vieil. : Gériatrie et psychologie & neuropsychiatrie du vieillissement  
  - Psychol Neuropsychiatr Vieil : Psychologie & NeuroPsychiatrie du vieillissement  
  - Retraite et société  
  - Current Alzheimer Research  
  - Rev Neurol (Paris) : Revue Neurologique  
  - GES : Gérontologie et Société  
  - Encephale : L'Encéphale  
  - Cortex  
  - Neuropsychology  
  - Soins Gerontol : Soins Gérontologie  
  - Annales Médico-Psychologiques : Annales Médico-Psychologiques, Revue Psychiatrique  
  - Arch Clin Neuropsychol : Archives of Clinical Neuropsychology  
  - Brain Cogn : Brain and Cognitio  
  - JDSAM : Journal de droit de la santé et de l'assurance maladie  
  - JCEN = Journal of Clinical and Experimental Neuropsychology  
  - L'Évolution Psychiatrique  
  - RDSS : RDSS. Revue de droit sanitaire et social  
  - Sciences Sociales et Santé

### 2. Auteurs actifs

Analyse des auteurs les plus actifs dans les revues SHS : Identification des auteurs avec le plus grand nombre de publications sur Alzheimer.

Sélectionner les colonnes essentielles pour faciliter notre analyse (docid, publicationDateY_i, docType_s, language_s, domain_s, primaryDomain_s, openAccess_bool, submitType_s, journalTitle_s, journalPublisher_s, authFullName_s, title_s, subTitle_s, citationRef_s, publicationDate_s).

```{r, include=TRUE}
revues_shs_auteur <- revues_shs_alzheimer %>% select(1:14,26) 
```

Distinguer les auteurs de chaque publication.

```{r, include=TRUE}
revues_shs_auteur$AutList <- strsplit(revues_shs_auteur$authFullName_s, ",")
```

Créer une autre colonne pour caculer le nombre d'auteurs de chaque publication.

```{r, include=TRUE}
revues_shs_auteur$nbAut <- NA

for(i in 1:length(revues_shs_auteur$AutList)){
  revues_shs_auteur$nbAut[[i]] <- length(revues_shs_auteur$AutList[[i]])}
```

Dupliquer chaque lignes de dataframe le nombre de fois spécifié dans la colonne 'nbAut' et attribuer des valeurs d'ordre. Si la valeur 'docid' de la ligne actuelle est identique à celle de la ligne précédente, alors la valeur d'ordre de la ligne actuelle est définie comme étant égale à la valeur d'ordre de la ligne précédente augmentée de 1. Sinon, la valeur d'ordre de la ligne actuelle est définie comme étant 1.

```{r, include=TRUE}
revues_shs_auteur <- revues_shs_auteur[rep(1:nrow(revues_shs_auteur), 
                                           revues_shs_auteur$nbAut),]

revues_shs_auteur$ordre <- NA

revues_shs_auteur$ordre[1] <- 1

for(i in 2:length(revues_shs_auteur$ordre)){
  {if(revues_shs_auteur$docid[i]==revues_shs_auteur$docid[i-1]){ revues_shs_auteur$ordre[i] <- revues_shs_auteur$ordre[i-1]+1 } else (revues_shs_auteur$ordre[i] <- 1)}
}
```

Extraire la valeur correspondante à la position de la colonne ordre dans la colonne 'AutList' de chaque ligne et l'assigner à la colonne 'Autuer'.

```{r, include=TRUE}
revues_shs_auteur$Auteur <- NA

for(i in 1:length(revues_shs_auteur$ordre)){
  revues_shs_auteur$Auteur[i] <- revues_shs_auteur$AutList[[i]][revues_shs_auteur$ordre[i]]}
```

Créer un tableau pour voir le nom des auteurs et leur nombre de publications. Après avoir réarrangé en ordre de sa fréquentation, on sélectionne les dix auteurs les plus actifs.

```{r, include=TRUE}
aut_actif <- table(revues_shs_auteur$Auteur)
aut_actif_df <- data.frame(aut = names(aut_actif), frequentation = as.numeric(aut_actif)) 
top_aut <- aut_actif_df[order(aut_actif_df$frequentation, decreasing = TRUE), ]
top_10 <- head(top_aut, 10)
```

Afin de comprendre les disciplines principales des dix auteurs, nous recherchons sur Internet leurs informations professionnelles et les ajouter dans une nouvelle colonne.

```{r, include=TRUE}
aut_discipline <- c("Psychologie", "Psychologie", "Gérontologie", "Gérontologie", "Gérontologie", "Gérontologie", "Psychologie", "Philosophie", "Psychologie", "Psychologie")

top_10$Discipline <- factor(aut_discipline, levels = c("Psychologie", "Gérontologie", "Philosophie"))
```

```{r, include=TRUE}
ggplot(top_10, aes(x = reorder(aut, -frequentation), y = frequentation, fill = Discipline)) +
  geom_bar(stat = "identity") + 
  scale_fill_manual(values = c("Psychologie" = "#1B9E77", "Gérontologie" = "#D95F02", "Philosophie" = "#7570B3")) +
  labs(title = "Les auteurs les plus actifs", x = "Auteur", y = "Nombre de publications") + 
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```
  
Mohamad El Haj, dans le domaine de psychologie, est l'auteur le plus actif avec 38 publications sur la maladie d'alzheimer. Entre les dix auteurs, cinq personnes contribuent au domaine de psychologie, quatre au gérontologie et un à la philosophie.

  
  Nous filtrons les cinq meilleurs auteurs et montrons les revues dans lesquelles ils ont écrit leurs recherches.

```{r, include=FALSE}
library(kableExtra)
```
```{r, include=TRUE}
aut_rev <- revues_shs_auteur %>% 
  filter(Auteur == "Mohamad El Haj" | 
           Auteur == "Philippe Allain" |
           Auteur == "Cédric Annweiler" |
           Auteur == "Jean-Pierre Jacus" |
           Auteur == "Karim Gallouj")

aut_rev <- revues_shs_auteur %>%
  filter(Auteur %in% c("Mohamad El Haj", "Philippe Allain", "Cédric Annweiler", "Jean-Pierre Jacus", "Karim Gallouj")) %>%
  distinct(Auteur, journalTitle_s) %>%
  group_by(Auteur) %>%
  summarize(Journals = paste(journalTitle_s, collapse = ", "))

```

```{r, include=TRUE}
kbl(aut_rev, caption = "Dans quelle revue les auteurs actifs ont-ils publié ?") %>%
  kable_styling(full_width = FALSE) %>%
  row_spec(0, bold = TRUE, color = "white", background = "skyblue")
```

### 3. Répartition des langues des publications des revues

```{r, include=TRUE}
# Créer une table des langues
langue_pays_freq <- revues_shs_alzheimer %>%
  count(language_s) %>%
  arrange(desc(n))
view(revues_shs_alzheimer)
# Créer la visualisation avec un graphique à barres empilées
ggplot(langue_pays_freq, aes(x = fct_reorder(language_s, n), y = n)) +
  geom_bar(stat = "identity", fill = "#E6AB02", color = "black") +  # Modifier les couleurs
  labs(
    title = "Répartition des langues des publications SHS sur la maladie d'Alzheimer",
    x = "Langue",
    y = "Nombre de publications",
  ) +
  geom_text(aes(label = n), vjust = -0.5, color = "black", size = 3, fontface="bold")+
  theme_minimal() +
  theme(axis.text.x = element_text(hjust = 1))
```


### 4. Nuage de mots des titres des revues SHS

Création d'un nuage de mots basé sur les titres des publications pour visualiser les termes les plus fréquents. Les mots "Alzheimer", "maladie" et "disease" sont exclus pour l'analyse. 


```{r, include=FALSE}
library(wordcloud) 
library(wordcloud2)
library(tm)
library(SnowballC) 
library(RColorBrewer)
library(viridis)
library(grid)
library(gridExtra)
```

**Les publications françaises**  

Après avoir créé un corpus pour analyser le texte, nous mettons les caractères en minuscules et enlevons les nombres, les stopwords, les trois mois basiques et les espaces vides.

```{r, include=TRUE, warning=FALSE}
revues_shs_alzheimer_fr <- revues_shs_alzheimer %>% filter(language_s == "fr")
nuage_corpus_fr <- Corpus(VectorSource(revues_shs_alzheimer_fr$title_s))

nuage_corpus_clean_fr<-tm_map(nuage_corpus_fr,tolower)
nuage_corpus_clean_fr<-tm_map(nuage_corpus_clean_fr,removeNumbers)
nuage_corpus_clean_fr<-tm_map(nuage_corpus_clean_fr,removeWords,stopwords("english"))
nuage_corpus_clean_fr<-tm_map(nuage_corpus_clean_fr,removeWords,stopwords("fr"))
nuage_corpus_clean_fr<-tm_map(nuage_corpus_clean_fr,removeWords, c("alzheimer", "maladie", "disease"))
nuage_corpus_clean_fr <- tm_map(nuage_corpus_clean_fr, content_transformer(function(x) gsub("[[:punct:]]", "", x)))
nuage_corpus_clean_fr<-tm_map(nuage_corpus_clean_fr,removePunctuation)
nuage_corpus_clean_fr<-tm_map(nuage_corpus_clean_fr,stripWhitespace)
```
  
  Le mot 'personnes' est le plus couramment utilisé dans les titres des publications françaises. Cependant, étant donné que ce mot est neutre, il est préférable de faire référence aux mots en violet les plus utilisés en deuxième place, tels que 'clinique', 'cognitive', 'mémoire', etc. Il existe également des mots anglais tels que 'mild' et 'care', mais c'est parce que les auteurs ont directement utilisé ces termes anglais dans les titres en français.

*Exemple : "Conscience des troubles dans la maladie d’Alzheimer et le mild cognitive impairment"*


```{r, include=TRUE, warning=FALSE}
wordcloud(nuage_corpus_clean_fr, max.words = 50, min.freq = 1,
          colors = brewer.pal(8, "Dark2"), rot.per = 0.35)
grid.text("Les mots-clés français les plus utilisés", x = 0.5, y = 0.98, 
          gp = gpar(fontsize = 15, fontface = "bold"))

```




**Les publications anglaises**
```{r, include=TRUE, warning=FALSE}
revues_shs_alzheimer_en <- revues_shs_alzheimer %>% filter(language_s == "en")

nuage_corpus_en <- Corpus(VectorSource(revues_shs_alzheimer_en$title_s))
nuage_corpus_clean_en<-tm_map(nuage_corpus_en,tolower)
nuage_corpus_clean_en<-tm_map(nuage_corpus_clean_en,removeNumbers)
nuage_corpus_clean_en<-tm_map(nuage_corpus_clean_en,removeWords,stopwords("english"))
nuage_corpus_clean_en<-tm_map(nuage_corpus_clean_en,removeWords,stopwords("fr"))
nuage_corpus_clean_en<-tm_map(nuage_corpus_clean_en,removeWords, c("alzheimer", "maladie", "disease"))
nuage_corpus_clean_en <- tm_map(nuage_corpus_clean_en, content_transformer(function(x) gsub("[[:punct:]]", "", x)))
nuage_corpus_clean_en<-tm_map(nuage_corpus_clean_en,removePunctuation)
nuage_corpus_clean_en<-tm_map(nuage_corpus_clean_en,stripWhitespace)

```

  
  Nous pouvons observer plusieurs mots-clés importants qui représentent les sujets souvent recherchés dans les publications sur la maladie d'Alzheimer. Les mots les plus fréquemment utilisés sont 'memory' et 'cognitive', suivis de 'patients', 'impairment', 'mild', 'dementia', etc.

```{r, include=TRUE, warning=FALSE}
wordcloud(nuage_corpus_clean_en,max.words = 50, min.freq =1, colors = brewer.pal(8, "Dark2"), rot.per=0.35)
grid.text("Les mots-clés anglais les plus utilisés", x = 0.5, y = 0.98, 
          gp = gpar(fontsize = 15, fontface = "bold"))
```
  

### 5. Les institutions les plus actives dans la publications des articles des revues SHS

Nous allons supprimer les NA dans journalPublisher_s. Il faut les supprimer dans cette étape et non pas lors du nettoyage pour garder les lignes qui servent pour d'autres analyses.

```{r, include=TRUE}
revues_shs_alzheimer <- revues_shs_alzheimer %>%
  filter(!is.na(journalPublisher_s))
```

-   Cette ligne de code utilise la fonction **`filter`** pour exclure les lignes où la colonne **`journalPublisher_s`** a des valeurs **`NA`**. Cela nettoie le dataframe en retirant les observations avec des valeurs manquantes dans la colonne spécifiée.

    Nous allons maintenant sélectionner le top 10 des institutions de revue les plus actifs:

```{r, include=FALSE}
top_institutions <- revues_shs_alzheimer %>%
  count(journalPublisher_s) %>%
  arrange(desc(n)) %>%
  head(10)
```

Cette séquence de commandes utilise **`count`** pour compter le nombre d'occurrences de chaque éditeur de revue, puis **`arrange`** pour les trier par ordre décroissant en fonction du nombre de publications (**`n`**). Enfin, **`head(10)`** est utilisé pour sélectionner les 10 éditeurs avec le nb le plus élevé.

Nous allons maintenant créer une visualisation :

```{r, include=TRUE}
ggplot(top_institutions, aes(x = n, y = fct_reorder(journalPublisher_s, n))) +
  geom_bar(stat = "identity", fill = "#E6AB02", color = "black") +
  labs(
    title = "Institutions les plus actives dans les revues SHS",
    x = "Nombre de publications",
    y = "Institution"
  ) +
  scale_x_continuous(breaks = seq(0, max(top_institutions$n), by = 7)) +
  scale_y_discrete(labels = function(x) str_wrap(x, width = 40))+
  theme_minimal()
```

Les barres sont remplies avec la couleur **`#E6AB02`** qui fait partie de la palette Dark2.

Le choix d'afficher le nombre de publications par 7 permet de bien visualiser l'axe X autant plus que les derniers éditeurs ont publié 7 publications.






Analyse :

-   La catégorie la plus prédominante est "Article dans une revue", représentant environ 47.24% de l'ensemble des types de documents.

-   "Communication dans un congrès" représente une part significative, soit environ 19.78% de l'ensemble.

-   Les "Ouvrages" représentent environ 14.51% de l'ensemble, contribuant de manière notable.

-   Environ 10.19% des types de documents sont classés comme "Thèse", une part significative mais inférieure à d'autres catégories.

-   Les "Poster de conférence" constituent une part relativement faible, soit environ 2.99%.

-   La catégorie "Rapport" représente environ 2.28% de l'ensemble, une part modérée.

-   Environ 2.99% des types de documents sont classés comme "Autre publication scientifique".


### 6. Tendances annuelles de publications

Analyse des tendances de publication au fil des années : Identification des tendances globales et par type de publication.

Nous allons calculer le nombre de publications pour chaque année et type de document en créant un dataframe de résumé.

```{r, include=TRUE}
tendances_publication <- dataset_shs_alzheimer %>%
  group_by(publicationDateY_i, docType_s) %>%
  summarise(count = n()) %>%
  filter(!(docType_s %in% c("UNDEFINED")))
view(tendances_publication)
```

Visualisation des données avec la palette de couleurs viridis :

```{r, include=TRUE}
ggplot(tendances_publication, aes(x = publicationDateY_i, y = count, fill = docType_s)) +
  geom_bar(stat = "identity", position = "stack", width = 0.7, color = "black") +
  scale_fill_brewer(palette = "Dark2") +
  labs(
    title = "Tendances des publications SHS sur la maladie d’Alzheimer depuis 1998",
    x = "Année de publication",
    y = "Nombre de publications",
    fill = "Type de document"
  ) +
  theme_minimal() +
  scale_x_continuous(breaks = unique(tendances_publication$publicationDateY_i), labels = unique(tendances_publication$publicationDateY_i)) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1, size=7))

```

Analyse :

-   En 1998, les publications exclusivement dédiées à la maladie d'Alzheimer dans le domaine des SHS étaient toutes des ouvrages.

-   À partir de 2001, chaque année (à l'exception de 2004), des articles portant sur la maladie d'Alzheimer dans le domaine des SHS ont été publiés dans des revues.

-   L'année 2017 a enregistré un pic dans le nombre de publications dédiées à la maladie d'Alzheimer dans le domaine des SHS.

-   En 2023, il y a eu une augmentation significative du nombre d'ouvrages publiés par rapport aux années précédentes dans le domaine des SHS traitant de la maladie d'Alzheimer.


### 7. Analyse de la répartition des publications SHS

Tout d'abord, il faut installer et charger le package ggrepel s'il n'est pas déjà installé

```{r, include=TRUE}
library(ggrepel)
```

Nous allons créer un résumé des comptes et pourcentages pour chaque catégorie docType_s :

```{r, include=TRUE}
count_data <- dataset_shs_alzheimer %>%
  count(docType_s) %>%
  mutate(percentage = prop.table(n) * 100)

```

Nous allons créer un camembert avec des étiquettes de pourcentage à l'extérieur et des flèches :

```{r, include=TRUE}

ggplot(count_data, aes(x = "", y = n, fill = docType_s)) +
  geom_bar(width = 1, stat = "identity", color = "white") +
  
  geom_label_repel(aes(
    label = scales::percent(percentage / 100),
    fill = docType_s
  ),
  position = position_stack(vjust = 0.5),
  box.padding = 0.5,
  point.padding = 0.3,
  arrow = arrow(length = unit(0.02, 'npc')),
  show.legend = FALSE) +
  
  coord_polar("y", start = 0) +
  scale_fill_brewer(palette = "Dark2") +
  theme_void() +
  theme(axis.text = element_blank()) +
  labs(title = "Répartition des types de publications sur la maladie d'Alzheimer",
       fill = "Type de publication")

```




# Interprétations des résultats

# Création d'un taggage via Zotero

# Création d'une bibliographie sur Zotero

# Conclusion

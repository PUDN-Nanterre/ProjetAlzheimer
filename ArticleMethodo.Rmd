---
title: "Gestion de projet (collaboration avec PUDN)"
author: "Remaissa BENDIB, Hanjoon KO, Gracia YAN"
date: "`r Sys.Date()`"
output:
  html_document:
    df_print: paged
  pdf_document: default
---

# Introduction

Ce projet du cours de Gestion de projet de la formation en Information et Communication à l’Université Paris Nanterre a pour objectif de collaborer avec des organisations françaises pour un projet spécifique. Notre projet, en collaboration avec PUDN (Plateforme Universitaire de Données de Nanterre), vise à gérer les recherches sur la maladie d'Alzheimer archivées sur HAL. En utilisant l'API de HAL, nous téléchargeons les données des articles et les nettoyons pour analyser les recherches sur la maladie ainsi que les revues dans lesquelles elles sont publiées.

### **Introduction aux librairies**

Le succès d'une analyse de données en R dépend souvent de l'utilisation des librairies appropriées. Les librairies fournissent des fonctions, des outils et des méthodes spécifiques qui facilitent la manipulation des données, la visualisation et l'analyse statistique. Dans ce guide, nous allons utiliser plusieurs librairies, parmi lesquelles, on retrouve :

#### **1. Librairie permettant le chargement des données :**

-   **Readr :** Utilisée pour lire et charger des données tabulaires à partir de fichiers plats, offrant une méthode efficace pour **importer des données dans R.**

#### **2. Librairie de nettoyage de données :**

-   **Labelled :** Utile pour **travailler avec des données étiquetées**.

-   **Stringr :** Offre des fonctionnalités avancées pour **la manipulation de chaînes de caractères,** particulièrement utile lors du nettoyage et de la préparation des données.

-   **Textcat** : Détecte la langue d'un document textuel. Elle est basée sur des modèles de langage statistiques pour déterminer la probabilité qu'un document soit écrit dans une langue spécifique.

#### **3. Statistiques descriptives rapides :**

-   **Skimr :** Génère des statistiques descriptives rapides pour l'ensemble du jeu de données, offrant un **aperçu rapide des variables et de leur distribution.**

#### **4. Manipulation et visualisation des données :**

-   **Tidyverse :** Collection de librairies (ggplot2, dplyr, tidyr, etc.) pour **le nettoyage, la manipulation et la visualisation des données**. Adoptez une approche cohérente et "propre" de l'analyse des données.

-   **GGPlot2 :** Librairie de visualisation pour **créer une variété de graphiques et de diagrammes,** particulièrement utile pour représenter visuellement les tendances, les distributions et les relations dans les données.

-   **Ggrepel :** Extension de ggplot2 utilisée pour **éviter les chevauchements de texte dans les graphiques**, améliorant la lisibilité des étiquettes.

-   **KableExtra :** Permet de **créer des tableaux personnalisables** en formatant les sorties de dataframes, particulièrement utile lors de la présentation des résultats.

-   **Wordcloud et Wordcloud2 :** Utilisées pour **générer des nuages de mots basés** sur les titres des publications, offrant une visualisation des termes les plus fréquemment utilisés.

-   **RColorBrewer et Viridis :** Fournissent des **palettes de couleurs** pour les visualisations graphiques, améliorant la lisibilité et l'esthétique des graphiques.

#### **5. Prétraitement du texte :**

-   **TM (Text Mining) et SnowballC :** Utilisées pour le prétraitement du texte, notamment pour la création d'un corpus et le **nettoyage des données textuelles.**

    En garantissant que nous avons installé et chargé ces librairies au début de notre projet, nous nous assurons d'avoir toutes les fonctionnalités nécessaires pour effectuer une analyse complète.

```{r, include=FALSE}

# Chargement des librairies nécessaires
library(readr)      # Pour lire et charger des données tabulaires
library(labelled)   # Pour travailler avec des données étiquetées
library(questionr)  # Fournit des outils d'analyse de données en sciences sociales
library(stringr)    # Pour la manipulation avancée de chaînes de caractères
library(tidyverse)  # Collection de librairies (dplyr, ggplot2, tidyr) pour une analyse cohérente
library(skimr)      # Génère des statistiques descriptives rapides pour l'ensemble du jeu de données
library(ggplot2)    # Création de graphiques et de visualisations
library(ggrepel)    # Évite les chevauchements de texte dans les graphiques
library(kableExtra) # Personnalisation des tableaux
library(wordcloud)   # Nuage de mots
library(wordcloud2)  # Nuage de mots interactif
library(textcat) #  Classification automatique de texte 
```

# Présentation HAL

# Extraction à partir de l'API

Reprendre la méthodo de Abir Gabriel en la modifiant ?

<https://gitlab.huma-num.fr/bchauvel/biblioalzheimer/-/blob/main/1_Extraction/FicheMethode_requeteHAL.Rmd?ref_type=heads>

Pour mener à bien notre analyse, nous commençons par extraire des données pertinentes à partir de l'API HAL. Ce dernier est une ressource centralisée regroupant des publications scientifiques en libre accès.

Nous utilisons les requêtes suivantes pour obtenir les informations nécessaires :

```{r}

# Import des données dans R à partir de l'API HAL
url <- "https://api.archives-ouvertes.fr/search/hal/?q=alzheimer&rows=7000&wt=csv&indent=true&fl=docid,publicationDateY_i,docType_s,language_s,domain_s,primaryDomain_s,openAccess_bool,submitType_s,journalTitle_s,journalPublisher_s,authFullName_s,title_s,subTitle_s,citationRef_s,doiId_s,issue_s,journalIssn_s,volume_s,source_s,licence_s,files_s,journalTitleAbbr_s,title_st,submitType_s,type_s,page_s,publicationDate_s,keyword_s,en_keyword_s,fr_keyword_s,abstract_s,en_abstract_s,fr_abstract_s&sort=publicationDateY_i%20desc"

options(timeout=600) # pour forcer le temps limite de chargement (si faible connexion internet)
download.file(url, destfile = "AlzheimerHAL.csv")

url <- "https://api.archives-ouvertes.fr/search/hal/?q=alzheimer&rows=7000&wt=bibtex&indent=true&fl=docid,publicationDateY_i,docType_s,language_s,domain_s,primaryDomain_s,openAccess_bool,submitType_s,journalTitle_s,journalPublisher_s,authFullName_s,title_s,subTitle_s,citationRef_s,doiId_s,issue_s,journalIssn_s,volume_s,source_s,licence_s,files_s,journalTitleAbbr_s,title_st,submitType_s,type_s,page_s,publicationDate_s,keyword_s,en_keyword_s,fr_keyword_s,abstract_s,en_abstract_s,fr_abstract_s&sort=publicationDateY_i%20desc"

options(timeout=600) # pour forcer le temps limite de chargement (si faible connexion internet)
download.file(url, destfile = "AlzheimerHAL.bib")

# Le fichier csv est ensuite importé dans la session R sous forme de tableau de données
dataset_alzheimer <- read.csv("AlzheimerHAL.csv")
```

# Nettoyage des données

## Étiquetage des variables

Les noms des variables du dataframe sont identiques aux champs sélectionnés lors de l'export depuis l'API. Pour une meilleure compréhension et documentation, nous utilisons la fonction **`var_label`** du package **`labelled`** pour ajouter des étiquettes informatives à chaque variable.

```{r, include=TRUE}

# Affichage des noms des variables
names(dataset_alzheimer)
```

On ajoute des étiquettes aux noms de variables (avec le package labelled), pour les documenter et limiter les risques de mauvaises interprétations.

```{r, include=TRUE}

# Ajout des étiquettes aux noms de variables
var_label(dataset_alzheimer) <- list(
  docid = "Identifiant HAL du dépôt", 
  publicationDateY_i = "Date de publication : année", 
  docType_s = "Type de document", 
  language_s = "Langue du document (code ISO 639-1 (alpha-2))", 
  domain_s = "Codes domaines du document", 
  primaryDomain_s = "Domaine primaire", 
  openAccess_bool = "publication en open access", 
  submitType_s = "Type de dépôt", 
  journalTitle_s = "Revue : Titre", 
  journalPublisher_s = "Revue : Editeur", 
  authFullName_s = "Auteur : Nom complet", 
  title_s = "Titres", 
  subTitle_s = "Sous-titre", 
  citationRef_s = "Citation abrégée", 
  doiId_s = "Identifiant DOI", 
  issue_s = "Numéro de revue", 
  journalIssn_s = "Revue : ISSN", 
  volume_s= "Volume", 
  source_s= "Source", 
  licence_s= "Droit d'auteur associé au document", 
  files_s= "URL des fichiers", 
  journalTitleAbbr_s= "Revue : Titre abrégé", 
  title_st= "Titres (sans les mots vides)", 
  type_s= "Type", page_s= "Pagination", 
  publicationDate_s= "Date de publication", 
  keyword_s = "Mots-clés", 
  en_keyword_s = "Mots-clés en anglais", 
  fr_keyword_s = "Mots-clés en français", 
  abstract_s = "Résumé", 
  en_abstract_s = "Résumé en anglais", 
  fr_abstract_s = "Résumé en français")
```

## Étiquetage des modalités

Afin de rendre les modalités plus explicites, par exemple, pour le type et la langue du document, nous utilisons la fonction **`val_labels`** pour attribuer des libellés descriptifs aux niveaux des variables catégorielles.

```{r, include=TRUE}
val_labels(dataset_alzheimer$docType_s) <- c(
  "Article dans une revue" = "ART", 
  "Article de blog scientifique" = "BLOG", 
  "Communication dans un congrès" = "COMM", 
  "Chapitre d'ouvrage" = "COUV", 
  "N°spécial de revue/special issue" = "ISSUE", 
  "Cours" = "LECTURE", 
  "Autre publication scientifique" = "OTHER", 
  "Ouvrages" = "OUV", 
  "Brevet" = "PATENT", 
  "Poster de conférence" = "POSTER", 
  "Rapport" = "REPORT", 
  "Thèse" = "THESE", 
  "Vidéo" = "VIDEO")
```

Ces étapes d'étiquetage des variables et des modalités **renforcent la compréhension du jeu de données et minimisent les risques de mauvaises interprétations** lors de l'analyse ultérieure.

Et pour finir cette partie, nous allons générer une table présentant les fréquences de chaque type de documents dans notre dataframe. Le but étant d'avoir une vie d'ensemble sur la distribution de ces types.

```{r, include=TRUE}

# Affichage des fréquences des types de documents
freq(dataset_alzheimer$docType_s, sort = "dec", valid = FALSE, total = TRUE) %>% knitr::kable(caption = "Types de documents présents dans le corpus")

```

**Langue :**

Comme déjà fait avant, dans cette partie du code liées aux données de langue, nous allons encore utiliser la fonction 'val_labels' pour attribuer des étiquettes aux codes de langues.

```{r, include=TRUE}

# Étiquetage des langues
val_labels(dataset_alzheimer$language_s) <- c(
  "Allemand" = "de", 
  "Anglais" = "en", 
  "Espagnol" = "es", 
  "Français" = "fr", 
  "Portugais" = "pt", 
  "Ukrainien" = "uk")
```

```{r, include=TRUE}
# Affichage des fréquences des langues dans le corpus
freq(dataset_alzheimer$language_s, sort = "dec", valid = FALSE, total = TRUE) %>% 
  knitr::kable(caption = "Langue du document")
```

**Domaines :**

Maintenant, nous souhaitons afficher les codes domaines les plus fréquents dans le jeux de données. Pour ce faire, nous allons utiliser encore une fois la fonction **`freq()`**

```{r, include=TRUE}
freq(dataset_alzheimer$primaryDomain_s, sort = "dec", valid = FALSE, total = TRUE) %>% 
  head(20) %>% 
  knitr::kable(caption = "Domaines primaires détaillés (les 20 plus fréquents)")
```

On créé une variable `domaine_gpe`pour les regrouper les domaines.

Nous avons défini une expression régulière (**`"^chim|^info|^math|^phys|^scco|^sde|^sdu|^sdv|^shs|^spi|^stat"`**) qui spécifie les motifs que nous cherchons.

Chaque motif commence par le symbole **`^`** indiquant le début de la chaîne de caractères, suivi d'une abréviation de domaine (par exemple, **`chim`** pour chimie, **`info`** pour informatique, etc.).

Ces motifs sont regroupés avec le symbole **`|`**, qui fonctionne comme un "OU". Ainsi, la recherche correspond à des chaînes qui commencent par l'une des abréviations spécifiées.

```{r, include=TRUE}
mots <- "^chim|^info|^math|^phys|^scco|^sde|^sdu|^sdv|^shs|^spi|^stat"
```

Cette partie du code prépare la création d'une variable **`domaine_gpe`** en extrayant les débuts de chaînes de caractères basés sur des critères spécifiques. Le but étant de regrouper les domaines en catégories plus larges en fonction de leurs abréviations.

Maintenant, nous allons créer la colonne en question. Dans notre code, on spécifie que notre colonne contient les résultats de l'extraction des motifs spécifiés (**`pattern = mots`**) de la colonne existante **`primaryDomain_s`** (qui représente les domaines primaires).

```{r, include=TRUE}

#Création de nouvelle colonne
dataset_alzheimer$domaine_gpe <- str_extract(dataset_alzheimer$primaryDomain_s, pattern = mots)
```

Nous allons ajouté des étiquettes aux modalités de la variable nouvellement créée, **`domaine_gpe`** :

```{r, include=TRUE}

#Ajout des étiquettes aux modalités de domaine_gpe
val_labels(dataset_alzheimer$domaine_gpe) <- c( 
  "Chimie" = "chim", 
  "Informatique [cs]" = "info", 
  "Mathématiques [math]" = "math", 
  "Physique [physics]" = "phys", 
  "Économie et finance quantitative [q-fin]" = "qfin", 
  "Sciences cognitives" = "scco", 
  "Sciences de l'environnement" = "sde", 
  "Planète et Univers [physics]" = "sdu", 
  "Sciences du Vivant [q-bio]" = "sdv", 
  "Sciences de l'Homme et Société" = "shs", 
  "Sciences de l'ingénieur [physics]" = "spi", 
  "Statistiques [stat]" = "stat")
```

Et pour finir, obtenir les fréquences de la nouvelle variable :

```{r, include=TRUE}

#Création d'un tableau de fréquence
freq(dataset_alzheimer$domaine_gpe, sort = "dec", valid = FALSE) %>% 
  knitr::kable(caption = "Domaines primaires regroupés")
```

On sauvegarde les données et leur configuration dans le fichier "AlzheimerHAL.Rda" qui sera utilisé pour faire les analyses.

```{r, include=TRUE}

#Sauvegarde du jeux de données dans un fichier Rda
save(dataset_alzheimer, file = "AlzheimerHAL.Rda")
```

## Préparation des datasets spécifiques pour faire des analyses

-   [Étape 1:]{.underline} Préparation du dataset général

Dans cette étape, nous procéderons à la préparation du dataset général pour faire des visualisations sur les tendances générales.

-   [Étape 2:]{.underline} Création d'un sub-dataset spécialisé pour les revues

Dans cette étape, nous créerons un sub-dataset spécifique qui se concentre exclusivement sur les publications de type "revues". Cela permettra une analyse plus approfondie et ciblée sur ce sous-ensemble de données, facilitant ainsi l'extraction d'informations spécifiques liées aux revues.

-   [Etape 3]{.underline} : Enrichissement manuelle des données liées aux types de revues (cette étape viendra dans les analyses, car nous allons seulement enrichir le top20 des revues)

### Étape 1

Préparation du dataset général :

A partir du dataset initial, nous allons créer un sous-ensemble de données que nous allons appeler dataset_shs_alzheimer.

Notre filtrage des lignes est basé sur des conditions spécifiques : la fonction str_detect nous permettra de rechercher le motif "shs" ou "ssh" dans la colonne domain_s ou la colonne primaryDomain_s, avec la conversion en minuscules pour rendre la recherche insensible à la casse.

```{r, include=TRUE}

dataset_shs_alzheimer <- dataset_alzheimer %>%
  
  # Filtrage des lignes basé sur des conditions spécifiques
  filter(
    # Utilisation de str_detect pour rechercher le motif "shs" ou "ssh" dans la colonne domain_s
    # ou la colonne primaryDomain_s, avec la conversion en minuscules pour rendre la recherche insensible à la casse
    str_detect(tolower(domain_s), "shs|ssh") |
      str_detect(tolower(primaryDomain_s), "shs|ssh")
  )

```

Toujours dans l'optique de nettoyer nos données, vu que nous avons plusieurs types de publications, nous allons rassembler certaines avec d'autres.

Nous allons donc agréger nos données liées aux types de publications.

Les principales modifications incluent :

-   Regrouper "ISSUE" avec "ART" dans la catégorie "Article dans une revue".

-   Regrouper "PROCEEDINGS" avec "COMM" dans la catégorie "Communication dans un congrès".

-   Regrouper "HDR" avec "THESE".

-   Regrouper "UNDEFINED" avec d'autres catégories d'"Autre publication scientifique".

-   Regrouper "COUV" avec "Ouvrages".

```{r, include=TRUE}

# Agréger les données dans la colonne docType_s
dataset_shs_alzheimer <- dataset_shs_alzheimer %>%
  mutate(docType_s = case_when(
    docType_s %in% c("ART", "ISSUE") ~ "Article dans une revue",
    docType_s %in% c("BLOG") ~ "Article de blog scientifique",
    docType_s %in% c("COMM", "PROCEEDINGS") ~ "Communication dans un congrès",
    docType_s %in% c("LECTURE") ~ "Cours",
    docType_s %in% c("OTHER", "VIDEO", "TRAD", "UNDEFINED") ~ "Autre publication scientifique",
    docType_s %in% c("OUV", "COUV") ~ "Ouvrages",
    docType_s %in% c("PATENT") ~ "Brevet",
    docType_s %in% c("POSTER") ~ "Poster de conférence",
    docType_s %in% c("REPORT") ~ "Rapport",
    docType_s %in% c("THESE", "HDR") ~ "Thèse",
    TRUE ~ as.character(docType_s)  
  ))

```

### Étape 2

Cette étape consiste dans la création d'un dataset pour analyser les données liées aux revues.

```{r, include=TRUE}

#Affichage des données liées aux types de documents
unique_doctype <- unique(dataset_shs_alzheimer$docType_s) 
print(unique_doctype)
```

Vu que nous souhaitons nous concentrer sur les revues SHS, nous allons donc créer un nouveau dataset qui prend seulement les données liées aux revues.

Nous allons tout d'abord filtrer le jeu de données shs pour inclure uniquement les lignes où docType_s est "Article dans une revue" et où la colonne journalTitle_s contient les mots "Revue", "revue", "review" ou "Review" car il se peut que quelques revues ne soient pas indiquées comme étant des revues dans le type du document (undefined)

```{r, include=TRUE}
revues_shs_alzheimer <- dataset_shs_alzheimer %>%
  filter(
    docType_s %in% c("Article dans une revue") |
      str_detect(tolower(journalTitle_s), "revue|review")
  )
```

Nous avons 394 publications de revues.

Nous allons nettoyer la colonne de langue dans notre mini-dataset des revues. Nous allons uniquement effectuer le nettoyage de la langue ici, car notre analyse linguistique est spécifiquement axée sur les revues. Il est important de noter que nous avons choisi de nettoyer la colonne de langue en raison d'erreurs potentielles liées à la saisie des données sur HAL.

Pour nettoyer nos données de langue, nous allons tout d'abord les afficher pour avoir une vue sur l'existant :

```{r, include=TRUE}

# Créer une table des occurrences de chaque langue
language_counts <- table(revues_shs_alzheimer$language_s)

# Afficher la table
print(language_counts)

```

Pour nettoyer notre colonne de langue, nous allons utiliser la librarie textcat. Cette librairie basée sur des modèles NLP pré-entrainés permet la détection automatique de la langue d'un texte. Il s'agit plus précisemment d'un N_gramm de la base de donnés pour 26 langues basé sur le corpus miltilingue de l'initiative Corpus Européen. Pour plus d'informations, visitez : (<https://cran.r-project.org/web/packages/textcat/textcat.pdf>)

Notre approche consiste à détecter la langue du document à partir du titre, puis à la comparer avec la langue déjà saisie sur HAL (dans la colonne "language_s"). Nous procéderons ensuite à une vérification manuelle des données incohérentes, c'est-à-dire lorsque le résultat de la détection diffère de la langue enregistrée sur HAL. Cette vérification sera effectuée en consultant la publication en ligne pour assurer la précision des données.

```{r, include=FALSE}

# Utilisation de la fonction textcat pour détecter la langue des titres d'articles
revues_shs_alzheimer$language_verification <- sapply(revues_shs_alzheimer$title_s, textcat)

# Afficher les 20 premières lignes du DataFrame avec la nouvelle colonne language_verification
head(revues_shs_alzheimer, 20)

```

Nous allons créer une nouvelle colonne appelée 'language_verification'. Cependant, la bibliothèque renvoie le résultat en tant que 'french' pour le français et 'english' pour l'anglais. Étant donné que nous voulons comparer nos résultats avec ceux déjà saisis sur HAL, nous devons ajuster la sortie :

```{r, include=FALSE}

#Modifier les valeurs en "fr" ou "en" en fonction de la langue détectée
revues_shs_alzheimer$language_verification <- ifelse(revues_shs_alzheimer$language_verification == "french", "fr",
                                                     ifelse(revues_shs_alzheimer$language_verification == "english", "en", NA))

```

Nous allons comparer ces données à celles déjà saisies sur HAL (donc language_s)

```{r, include=TRUE}

#Filtrer les lignes où language_verification n'est pas égal à language_s
lignes_langue_différente <- revues_shs_alzheimer %>%
  mutate(language_verification = as.character(language_verification),
         language_s = as.character(language_s)) %>%
  filter(language_verification != language_s)

```

```{r, include=FALSE}

#Afficher les lignes
print(lignes_langue_différente)
```

Maintenant que nous avons les lignes qui ne correspondent pas, nous allons vérifier manuellement si c'est le cas ou non, puis nous allons corriger les incohérences.

Nous allons corriger les données incorrectes à partir de l'identifiant docid :

Il est à noter que nous avons choisi 'docid' car c'est l'identifiant unique, et donc il ne se reproduit pas dans chaque ligne du jeu de données. Il renvoie à chaque publication."

```{r, include=TRUE}

revues_shs_alzheimer <- revues_shs_alzheimer %>%
  mutate(language_s = case_when(
    docid %in% c(1806639, 1240621, 984498, 1240619, 4375787, 3355729, 4457542, 4380879, 3524754,
                 4047932, 4391680, 2440803, 4201490, 1584518, 3817981, 4330838, 3730267, 4331180,
                 4375866, 3730264, 3497539, 3730263, 4334832, 3726046, 3643277, 4226480, 4334932,
                 4375647) ~ "fr",
    TRUE ~ language_s
  ))

```

Nous avons également observé une incohérence dans les lignes concernant les éditeurs. L'éditeur Elsevier est parfois écrit Elsevier Masson ou simplement Elservier. Nous allons donc mettre à jour la colonne 'journalPublisher_s'.

Pour ce faire, nous allons utiliser la fonction **`mutate`** du package **`dplyr`** pour effectuer une transformation sur la colonne **`journalPublisher_s`** du dataframe **`revues_shs_alzheimer`**. La transformation est réalisée avec la fonction **`if_else`**, qui permet de remplacer les occurrences de "Elsevier" dans la colonne **`journalPublisher_s`** par "Elsevier Masson".

En d'autres termes, chaque fois que la valeur de la colonne **`journalPublisher_s`** est "Elsevier", elle est mise à jour avec la valeur "Elsevier Masson".

```{r, include=TRUE}

revues_shs_alzheimer <- revues_shs_alzheimer %>%   
  mutate(     journalPublisher_s = if_else(journalPublisher_s == "Elsevier", "Elsevier Masson", journalPublisher_s)   )
```

Nous allons remplir les lignes vides dans journalPublisher_s par des NA pour pouvoir retirer les NA lors de nos analyses :

```{r}
revues_shs_alzheimer$journalPublisher_s <- replace(revues_shs_alzheimer$journalPublisher_s, revues_shs_alzheimer$journalPublisher_s == "", NA)

# Créer et afficher la table des occurrences
table_journal_publisher <- table(revues_shs_alzheimer$journalPublisher_s)
#print(table_journal_publisher)
```

# Analyses pour répondre à nos questions

# Interprétations des résultats

### 1. Tendances annuelles de publications

Pour avoir une idée globale de la recherche et de la popularité de HAL, il est intéressant d’explorer l’évolution temporelle des publications sur le sujet de la maladie d’Alzheimer dans le domaine qui nous intéresse : les Sciences Humaines et Sociales.

Nous allons tout d'abord calculer le nombre de publications pour chaque année et type de document en créant un dataframe de résumé.

```{r, include=TRUE}

# Résumé des tendances de publication par année et type de document, en excluant les types "UNDEFINED".
tendances_publication <- dataset_shs_alzheimer %>%
  group_by(publicationDateY_i, docType_s) %>%
  summarise(count = n()) %>%
  filter(!(docType_s %in% c("UNDEFINED")))
view(tendances_publication)

```

Visualisation des données avec la palette de couleurs Dark2 :

```{r, include=TRUE}

# Création d'un graphique à barres empilées montrant les tendances des publications SHS sur la maladie d'Alzheimer depuis 1998.
ggplot(tendances_publication, aes(x = publicationDateY_i, y = count, fill = docType_s)) +
  geom_bar(stat = "identity", position = "stack", width = 0.7, color = "black") +  # Barres empilées
  scale_fill_brewer(palette = "Dark2") +  # Palette de couleurs
  labs(
    title = "Tendances des publications SHS sur la maladie d’Alzheimer depuis 1998",
    x = "Année de publication",
    y = "Nombre de publications",
    fill = "Type de document"
  ) +  # Titres et légendes
  theme_minimal() +  # Style minimal
  scale_x_continuous(breaks = unique(tendances_publication$publicationDateY_i), labels = unique(tendances_publication$publicationDateY_i)) +  # Ajustements de l'axe x
  theme(axis.text.x = element_text(angle = 45, hjust = 1, size=7))  # Ajustements des étiquettes de l'axe x

```

La première publication accessible sur HAL portant sur notre sujet est un ouvrage qui a été publié en 1998. Cette date, antérieure à celle de la création de HAL, montre que la référence de publication indiquée peut aller au-delà des limites de la plateforme. HAL peut donc être un espace d’archivage de recherches scientifiques pour les ressources de plus de 25 ans.

Entre 2001 et 2006, il y a peu de publications. Mais il y a systématiquement un ou plusieurs articles publiés (à l’exception de 2004).

Il y a un pic de publications en 2012. Cette date marque la fin du Plan Alzheimer 2008-2012, projet politique qui tente d’améliorer la recherche française sur la maladie d’Alzheimer. On peut penser que de nombreuses recherches se sont intéressées, à cette période, à la maladie. Les résultats ont ensuite été publiés sur HAL.

2017 est une année où HAL enregistre le plus grand nombre de publications sur son site dans cette discipline.

Les articles de revues sont globalement le type de document qui y est le plus déposé.

### 2. Analyse de la répartition des publications SHS

Pour visualiser toutes ces publications sous un autre angle, il est possible de regrouper ces données pour avoir une image résumant la répartition totale de ces publications en SHS sans l’aspect temporel. Pour cela, l’analyse sera représentée sous forme de camembert et gagne en lisibilité grâce aux couleurs et aux étiquettes de pourcentage.

Avant de créer le camambert, nous allons tout d'abord créer un résumé des comptes et pourcentages pour chaque catégorie docType_s :

```{r, include=TRUE}

# Calcul du nombre d'occurrences de chaque type de document et calcule le pourcentage par rapport au total.
count_data <- dataset_shs_alzheimer %>%
  count(docType_s) %>%  # Calcul du nombre d'occurrences par type de document
  mutate(percentage = prop.table(n) * 100)  # Ajout d'une colonne avec le pourcentage par rapport au total

```

Maintenant, nous allons créer un camembert avec des étiquettes de pourcentage à l'extérieur et des flèches :

```{r, include=TRUE}

# Utilisation de ggplot2 et ggrepel pour générer un graphique polarisé avec des étiquettes représentant les pourcentages.
ggplot(count_data, aes(x = "", y = n, fill = docType_s)) +
  geom_bar(width = 1, stat = "identity", color = "white") +  

  geom_label_repel(aes(
    label = scales::percent(percentage / 100),
    fill = docType_s
  ),
  position = position_stack(vjust = 0.5),
  box.padding = 0.5,
  point.padding = 0.3,
  arrow = arrow(length = unit(0.02, 'npc')),
  show.legend = FALSE) +  # Ajout des étiquettes avec les pourcentages

  coord_polar("y", start = 0) +  
  scale_fill_brewer(palette = "Dark2") +  # Définir la palette de couleurs
  theme_void() +  # Suppression des éléments du thème par défaut
  theme(axis.text = element_blank()) +  
  labs(title = "Répartition des types de publications sur la maladie d'Alzheimer",
       fill = "Type de publication")  # Ajout des légendes

```

Les barres sont remplies avec la couleur **`#E6AB02`** qui fait partie de la palette Dark2.

-   La catégorie la plus prédominante est "Article dans une revue", représentant environ 47.24% de l'ensemble des types de documents.

-   "Communication dans un congrès" représente une part significative, soit environ 19.78% de l'ensemble.

-   Les "Ouvrages" représentent environ 14.51% de l'ensemble, contribuant de manière notable.

-   Environ 10.19% des types de documents sont classés comme "Thèse", une part significative mais inférieure à d'autres catégories.

-   Les "Poster de conférence" constituent une part relativement faible, soit environ 2.99%.

-   La catégorie "Rapport" représente environ 2.28% de l'ensemble, une part modérée.

-   Environ 2.99% des types de documents sont classés comme "Autre publication scientifique".

    En d'autres termes, le grand nombre d’articles de revues est plus parlant dans ce graphique en comparaison au précédent. Ici, on peut affirmer que presque 50% des publications sur l’Alzheimer, en SHS, disponibles sur HAL, sont des articles de revues.

    Cependant, ce pourcentage est le résultat des dépôts sur HAL. Il n’est pas représentatif de la recherche globale dans ce domaine puisqu’il ne s’agit que d’une seule base de données pour la recherche scientifique. Il serait intéressant de compléter ces recherches et de les comparer avec des données sur d’autres sites comme Cairn.

    En deuxième position, on voit que les communications dans des congrès sont davantage publiés que les ouvrages. On peut supposer que le dépôt sur HAL, une plateforme ouverte, n’incite pas les auteurs à soumettre leurs livres qu’il est possible d’acheter.

### 3. Top des revues les plus actives

Pour analyser les revues les plus actives en SHS, c'est-à-dire celles qui publient le plus de publications, couvrant les domaines principaux ou secondaires, ou ayant publié des publications SHS, nous avons trié, dans l'ordre croissant, 20 revues :

```{r, include=TRUE}

# Analyse des revues les plus actives
top_revues <- revues_shs_alzheimer %>%
  count(journalTitle_s) %>%  # Calcul du nombre de publications par revue
  arrange(desc(n)) %>%  # Tri par ordre décroissant du nombre de publications
  head(20)  # Sélection des 20 revues les plus actives
```

Comme déjà mentionné lors de la préparation du mini-dataset des revues, nous allons enrichir nos données. Pour ce faire, nous allons associer à chaque revue un domaine principal en cherchant via le moteur de recherche HAL, dans quels domaines la revue publiait le plus.

Voici comment nous allons procéder, étape par étape :

**1.Recherche du nom de la revue sur le moteur HAL :** On va utiliser le nom de la revue, par exemple, "Retraite et société", pour effectuer une recherche avancée sur le moteur HAL en spécifiant qu'il s'agit d'une revue.

**2. Filtration** **par sujet (Subject field) :** Une fois que nous avons les résultats de la recherche, à savoir les documents publiés par cette revue sur HAL, nous allons utiliser le ruban de filtration à gauche de la page de résultats. Nous allons rechercher la section "Subject field" dans le ruban de filtrage.

**3.** **Identification du domaine principal :** Dans la section "Subject field", nous allons examiner la fréquence de publications de la revue dans chaque domaine pour identifier le domaine dans lequel la revue publie le plus fréquemment.

```{r, include=TRUE}

# Enrichissement des données manuellement via HAL
top_revues <- top_revues %>%
  mutate(domaine_revue = case_when(
    journalTitle_s == "Journal of Alzheimer's Disease" ~ "Sciences du Vivant",
    journalTitle_s == "NPG: Neurologie - Psychiatrie - Gériatrie" ~ "SHS",
    journalTitle_s == "Gériatrie et psychologie & neuropsychiatrie du vieillissement" ~ "Sciences du Vivant",
    journalTitle_s == "Psychologie & NeuroPsychiatrie du vieillissement" ~ "Sciences du Vivant",
    journalTitle_s == "Retraite et société" ~ "SHS",
    journalTitle_s == "Current Alzheimer Research" ~ "Sciences du Vivant",
    journalTitle_s == "Revue Neurologique" ~ "Sciences du Vivant",
    journalTitle_s == "Gérontologie et Société" ~ "SHS",
    journalTitle_s == "L'Encéphale" ~ "Sciences du Vivant",
    journalTitle_s == "Cortex" ~ "Sciences du Vivant",
    journalTitle_s == "Neuropsychology" ~ "Sciences cognitives",
    journalTitle_s == "Soins Gérontologie" ~ "Sciences du Vivant",
    journalTitle_s == "Annales Médico-Psychologiques, Revue Psychiatrique" ~ "SHS",
    journalTitle_s == "Archives of Clinical Neuropsychology" ~ "Sciences cognitives",
    journalTitle_s == "Brain and Cognition" ~ "Sciences du Vivant",
    journalTitle_s == "Journal de droit de la santé et de l'assurance maladie" ~ "SHS",
    journalTitle_s == "Journal of Clinical and Experimental Neuropsychology" ~ "Sciences cognitives",
    journalTitle_s == "L'Évolution Psychiatrique" ~ "SHS",
    journalTitle_s == "RDSS. Revue de droit sanitaire et social" ~ "SHS",
    journalTitle_s == "Sciences Sociales et Santé" ~ "SHS",
    TRUE ~ "Autre"
  ))

```

Nous allons ajouté une nouvelle colonne appelée **`Abbréviation_revues`** au dataframe **`top_revues`**. Chaque abréviation est attribuée à une revue spécifique. Cela permet identification rapide des revues sans avoir à utiliser les noms complets de chaque revue.

**Voici l'abbréviation des titres de revues que nous allons utiliser :**\
- J Alzheimers Dis : Journal of Alzheimer's Disease\
- NPG : NPG: Neurologie - Psychiatrie - Gériatrie\
- Gériatr. psychol. Neuropsychiatr. vieil. : Gériatrie et psychologie & neuropsychiatrie du vieillissement\
- Psychol Neuropsychiatr Vieil : Psychologie & NeuroPsychiatrie du vieillissement\
- Retraite et société\
- Current Alzheimer Research\
- Rev Neurol (Paris) : Revue Neurologique\
- GES : Gérontologie et Société\
- Encephale : L'Encéphale\
- Cortex\
- Neuropsychology\
- Soins Gerontol : Soins Gérontologie\
- Annales Médico-Psychologiques : Annales Médico-Psychologiques, Revue Psychiatrique\
- Arch Clin Neuropsychol : Archives of Clinical Neuropsychology\
- Brain Cogn : Brain and Cognitio\
- JDSAM : Journal de droit de la santé et de l'assurance maladie\
- JCEN = Journal of Clinical and Experimental Neuropsychology\
- L'Évolution Psychiatrique\
- RDSS : RDSS. Revue de droit sanitaire et social\
- Sciences Sociales et Santé

```{r, include=TRUE}

# Ajout des abbréviations aux noms des revues 
top_revues$Abbréviation_revues <- c("J Alzheimers Dis","NPG","Gériatr. psychol. Neuropsychiatr. vieil.","Psychol Neuropsychiatr Vieil","Retraite et société","Current Alzheimer Research","Rev Neurol (Paris)","GES","Encephale","Cortex","Neuropsychology","Soins Gerontol","Annales Médico-Psychologiques","Arch Clin Neuropsychol","Brain Cogn","JDSAM","JCEN","L'Évolution Psychiatrique","RDSS","Sciences Sociales et Santé")
```

Maintenant nous allons créer notre graphique des revues les plus actives qui mentionne également le domaine des revues :

```{r, include=TRUE}

# Création d'un graphique à barres des revues les plus actives avec domaine_revue
ggplot(top_revues, aes(x = n, y = fct_reorder(Abbréviation_revues, n), fill = domaine_revue)) +
  geom_bar(stat = "identity", color = "black") +
  labs(
    title = "Revues les plus actives",
    x = "Nombre de publications",
    y = "Revue",
    fill = "Domaine de revue"
  ) +
  scale_fill_brewer(palette = "Dark2") +
  scale_y_discrete(labels = function(x) str_wrap(x, width = 28)) +
  theme_minimal()

```

Cette méthode d'enrichissemnt de données manuelle nous a permis de segmenter les domaines mais le manque d’information fournie au préalable sur la spécialité de ces revues pose un problème.

En effet, cette catégorisation par nos soins ne représente pas la réalité de ces revues.

Par exemple, le Journal de droit de la santé et de l’assurance maladie est très présent sur HAL dans le domaine SHS alors qu’il s’agit d’une revue qui porte davantage sur le droit et la médecine. Pour pallier ces erreurs, les auteurs doivent publier davantage sur HAL pour essayer d’atteindre une représentativité plus juste.

En ce qui concerne notre graphe, d’après les 20 revues les plus actives, les trois domaines majeurs qui traitent du sujet de l’Alzheimer sur HAL sont les sciences cognitives, les sciences du vivant et les sciences humaines et sociales.

En s’intéressant aux noms de ces revues, on remarque une récurrence de ce qui relève du domaine de la psychologie.

### 4. Auteurs actifs

Nous allons analyser les auteurs les plus actifs dans les revues SHS. Le but étant d'identifier les auteurs avec le plus grand nombre de publications sur Alzheimer.

Nous allons sélectionner les colonnes essentielles pour faciliter notre analyse :

Il s'agit de : (docid, publicationDateY_i, docType_s, language_s, domain_s, primaryDomain_s, openAccess_bool, submitType_s, journalTitle_s, journalPublisher_s, authFullName_s, title_s, subTitle_s, citationRef_s, publicationDate_s).

```{r, include=TRUE}
revues_shs_auteur <- revues_shs_alzheimer %>% select(1:14,26) 
```

Nous allons distinguer les auteurs de chaque publication.

```{r, include=TRUE}
revues_shs_auteur$AutList <- strsplit(revues_shs_auteur$authFullName_s, ",")
```

Nous allons créer une autre colonne pour calculer le nombre d'auteurs de chaque publication.

```{r, include=TRUE}
revues_shs_auteur$nbAut <- NA

for(i in 1:length(revues_shs_auteur$AutList)){
  revues_shs_auteur$nbAut[[i]] <- length(revues_shs_auteur$AutList[[i]])}
```

Nous allons dupliquer chaque lignes de dataframe le nombre de fois spécifié dans la colonne 'nbAut' et attribuer des valeurs d'ordre. Si la valeur 'docid' de la ligne actuelle est identique à celle de la ligne précédente, alors la valeur d'ordre de la ligne actuelle est définie comme étant égale à la valeur d'ordre de la ligne précédente augmentée de 1. Sinon, la valeur d'ordre de la ligne actuelle est définie comme étant 1.

```{r, include=TRUE}
revues_shs_auteur <- revues_shs_auteur[rep(1:nrow(revues_shs_auteur), 
                                           revues_shs_auteur$nbAut),]

revues_shs_auteur$ordre <- NA

revues_shs_auteur$ordre[1] <- 1

for(i in 2:length(revues_shs_auteur$ordre)){
  {if(revues_shs_auteur$docid[i]==revues_shs_auteur$docid[i-1]){ revues_shs_auteur$ordre[i] <- revues_shs_auteur$ordre[i-1]+1 } else (revues_shs_auteur$ordre[i] <- 1)}
}
```

Nous allons extraire la valeur correspondante à la position de la colonne ordre dans la colonne 'AutList' de chaque ligne et l'assigner à la colonne 'Auteur'.

```{r, include=TRUE}
revues_shs_auteur$Auteur <- NA

for(i in 1:length(revues_shs_auteur$ordre)){
  revues_shs_auteur$Auteur[i] <- revues_shs_auteur$AutList[[i]][revues_shs_auteur$ordre[i]]}
```

Nous allons créer un tableau pour voir le nom des auteurs et leur nombre de publications. Après avoir réarrangé en ordre de sa fréquentation, on sélectionne les dix auteurs les plus actifs.

```{r, include=TRUE}
aut_actif <- table(revues_shs_auteur$Auteur)
aut_actif_df <- data.frame(aut = names(aut_actif), frequentation = as.numeric(aut_actif)) 
top_aut <- aut_actif_df[order(aut_actif_df$frequentation, decreasing = TRUE), ]
top_10 <- head(top_aut, 10)
```

Afin de comprendre les disciplines principales des dix auteurs, nous recherchons sur Internet leurs informations professionnelles et les ajouter dans une nouvelle colonne.

```{r, include=TRUE}
aut_discipline <- c("Psychologie", "Psychologie", "Gérontologie", "Gérontologie", "Gérontologie", "Gérontologie", "Psychologie", "Philosophie", "Psychologie", "Psychologie")

top_10$Discipline <- factor(aut_discipline, levels = c("Psychologie", "Gérontologie", "Philosophie"))
```

```{r, include=TRUE}
ggplot(top_10, aes(x = reorder(aut, -frequentation), y = frequentation, fill = Discipline)) +
  geom_bar(stat = "identity") + 
  scale_fill_manual(values = c("Psychologie" = "#1B9E77", "Gérontologie" = "#D95F02", "Philosophie" = "#7570B3")) +
  labs(title = "Les auteurs les plus actifs", x = "Auteur", y = "Nombre de publications") + 
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

Mohamad El Haj, dans le domaine de psychologie, est l'auteur le plus actif avec 38 publications sur la maladie d'alzheimer. Entre les dix auteurs, cinq personnes contribuent au domaine de psychologie, quatre au gérontologie et un à la philosophie.

Nous filtrons les cinq meilleurs auteurs et montrons les revues dans lesquelles ils ont écrit leurs recherches.

```{r, include=FALSE}
library(kableExtra)
```

```{r, include=TRUE}
aut_rev <- revues_shs_auteur %>% 
  filter(Auteur == "Mohamad El Haj" | 
           Auteur == "Philippe Allain" |
           Auteur == "Cédric Annweiler" |
           Auteur == "Jean-Pierre Jacus" |
           Auteur == "Karim Gallouj")

aut_rev <- revues_shs_auteur %>%
  filter(Auteur %in% c("Mohamad El Haj", "Philippe Allain", "Cédric Annweiler", "Jean-Pierre Jacus", "Karim Gallouj")) %>%
  distinct(Auteur, journalTitle_s) %>%
  group_by(Auteur) %>%
  summarize(Journals = paste(journalTitle_s, collapse = ", "))

```

```{r, include=TRUE}
kbl(aut_rev, caption = "Dans quelle revue les auteurs actifs ont-ils publié ?") %>%
  kable_styling(full_width = FALSE) %>%
  row_spec(0, bold = TRUE, color = "white", background = "skyblue")
```

On a établi le top 10 des auteurs les plus actifs en leur ajoutant une discipline majeure d’après leurs informations présentes sur d’autres sites Internet.

HAL ne donne pas directement les informations relatives aux champs disciplinaires des auteurs. Cette recherche doit se faire manuellement et peut être assez contraignante si on cherche à représenter plus de 20 auteurs. De plus, un auteur peut avoir plusieurs disciplines majeures, ce qui est difficilement représentable sur le graphique.

La psychologie représente la discipline principale de la moitié de ces auteurs. Cela semble assez vraisemblable puisqu’on a remarqué précédemment que les revues en rapport avec la psychologie sont très présentes dans nos recherches. Cette discipline se place avant la gérontologie (l’étude du vieillissement), ce qui semble assez logique étant donné la maladie dont il est question.

Il y a tout de même un aspect philosophique à travers un auteur ce qui montre la diversité, voire la complémentarité, des angles d’approche pour étudier la maladie d’Alzheimer.

### 5. Répartition des langues des publications des revues

Dans cette partie, nous explorons la diversité linguistique des publications dans le domaine des sciences humaines et sociales (SHS) liées à la maladie d'Alzheimer. L'analyse vise à comprendre la répartition des langues utilisées dans les revues académiques traitant de ce sujet spécifique.

La première étape consiste à créer une table des fréquences des langues présentes dans notre ensemble de données.

```{r, include=TRUE}

# Créer une table des langues
langue_pays_freq <- revues_shs_alzheimer %>%
  count(language_s) %>%
  arrange(desc(n))
view(revues_shs_alzheimer)
```

On crée la visualisation :

```{r, include=TRUE}

# Créer la visualisation avec un graphique à barres empilées
ggplot(langue_pays_freq, aes(x = fct_reorder(language_s, n), y = n)) +  # Utilisation de ggplot avec les données et l'esthétique des axes
  geom_bar(stat = "identity", fill = "#E6AB02", color = "black") +  # Ajout des barres empilées avec des couleurs spécifiées
  labs(
    title = "Répartition des langues des publications SHS sur la maladie d'Alzheimer",  # Ajout d'un titre au graphique
    x = "Langue",  # Nom de l'axe des x
    y = "Nombre de publications",  # Nom de l'axe des y
  ) +
  geom_text(aes(label = n), vjust = -0.5, color = "black", size = 3, fontface="bold") +  # Ajout des étiquettes de données au-dessus des barres
  theme_minimal() +  # Utilisation d'un thème minimal pour le graphique
  theme(axis.text.x = element_text(hjust = 1))  # Ajustement de l'emplacement du texte de l'axe des x

```

A partir des textes des publications au sein des revues portant sur la thématique de l’Alzheimer, on peut distinguer les différentes langues.

Alors que le nettoyage réalisé plus haut sur les langues nous montrait leur petite diversité (plus de 5 sur l’ensemble des données), celle-ci se réduit drastiquement lorsqu’il s’agit de publications dans une revue. Les articles sont en anglais principalement, puis en français. Il n’y en a qu’un en portugais.

On peut penser que HAL est davantage populaire dans la recherche francophone et anglophone. Tout en sachant que HAL ne permet pas de faire ces segmentations facilement et de manière pratique. En effet, il y a beaucoup d’erreurs de catégorisation. Des ressources en français se retrouvaient dans la partie anglaise et inversement.

Il a fallut corriger ces erreurs manuellement.

### 6. Nuage de mots des titres des revues SHS

Nous allons créer un nuage de mots basé sur les titres des publications pour visualiser les termes les plus fréquents. Les mots "Alzheimer", "maladie" et "disease" sont exclus pour l'analyse.

```{r, include=FALSE}
library(wordcloud) 
library(wordcloud2)
library(tm)
library(SnowballC) 
library(RColorBrewer)
library(viridis)
library(grid)
library(gridExtra)
```

**Les publications françaises**

Après avoir créé un corpus pour analyser le texte, nous mettons les caractères en minuscules et enlevons les nombres, les stopwords, les trois mois basiques et les espaces vides.

```{r, include=TRUE, warning=FALSE}
revues_shs_alzheimer_fr <- revues_shs_alzheimer %>% filter(language_s == "fr")
nuage_corpus_fr <- Corpus(VectorSource(revues_shs_alzheimer_fr$title_s))

nuage_corpus_clean_fr<-tm_map(nuage_corpus_fr,tolower)
nuage_corpus_clean_fr<-tm_map(nuage_corpus_clean_fr,removeNumbers)
nuage_corpus_clean_fr<-tm_map(nuage_corpus_clean_fr,removeWords,stopwords("english"))
nuage_corpus_clean_fr<-tm_map(nuage_corpus_clean_fr,removeWords,stopwords("fr"))
nuage_corpus_clean_fr<-tm_map(nuage_corpus_clean_fr,removeWords, c("alzheimer", "maladie", "disease"))
nuage_corpus_clean_fr <- tm_map(nuage_corpus_clean_fr, content_transformer(function(x) gsub("[[:punct:]]", "", x)))
nuage_corpus_clean_fr<-tm_map(nuage_corpus_clean_fr,removePunctuation)
nuage_corpus_clean_fr<-tm_map(nuage_corpus_clean_fr,stripWhitespace)
```

Le mot 'personnes' est le plus couramment utilisé dans les titres des publications françaises. Cependant, étant donné que ce mot est neutre, il est préférable de faire référence aux mots en violet les plus utilisés en deuxième place, tels que 'clinique', 'cognitive', 'mémoire', etc. Il existe également des mots anglais tels que 'mild' et 'care', mais c'est parce que les auteurs ont directement utilisé ces termes anglais dans les titres en français.

*Exemple : "Conscience des troubles dans la maladie d’Alzheimer et le mild cognitive impairment"*

```{r, include=TRUE, warning=FALSE}
wordcloud(nuage_corpus_clean_fr, max.words = 50, min.freq = 1,
          colors = brewer.pal(8, "Dark2"), rot.per = 0.35)
grid.text("Les mots-clés français les plus utilisés", x = 0.5, y = 0.98, 
          gp = gpar(fontsize = 15, fontface = "bold"))

```

**Les publications anglaises**

```{r, include=TRUE, warning=FALSE}
revues_shs_alzheimer_en <- revues_shs_alzheimer %>% filter(language_s == "en")

nuage_corpus_en <- Corpus(VectorSource(revues_shs_alzheimer_en$title_s))
nuage_corpus_clean_en<-tm_map(nuage_corpus_en,tolower)
nuage_corpus_clean_en<-tm_map(nuage_corpus_clean_en,removeNumbers)
nuage_corpus_clean_en<-tm_map(nuage_corpus_clean_en,removeWords,stopwords("english"))
nuage_corpus_clean_en<-tm_map(nuage_corpus_clean_en,removeWords,stopwords("fr"))
nuage_corpus_clean_en<-tm_map(nuage_corpus_clean_en,removeWords, c("alzheimer", "maladie", "disease"))
nuage_corpus_clean_en <- tm_map(nuage_corpus_clean_en, content_transformer(function(x) gsub("[[:punct:]]", "", x)))
nuage_corpus_clean_en<-tm_map(nuage_corpus_clean_en,removePunctuation)
nuage_corpus_clean_en<-tm_map(nuage_corpus_clean_en,stripWhitespace)

```

Nous pouvons observer plusieurs mots-clés importants qui représentent les sujets souvent recherchés dans les publications sur la maladie d'Alzheimer. Les mots les plus fréquemment utilisés sont 'memory' et 'cognitive', suivis de 'patients', 'impairment', 'mild', 'dementia', etc.

```{r, include=TRUE, warning=FALSE}
wordcloud(nuage_corpus_clean_en,max.words = 50, min.freq =1, colors = brewer.pal(8, "Dark2"), rot.per=0.35)
grid.text("Les mots-clés anglais les plus utilisés", x = 0.5, y = 0.98, 
          gp = gpar(fontsize = 15, fontface = "bold"))
```

En résumé, en approfondissant cet aspect linguistique, on a généré 2 nuages de mots à partir des termes utilisés dans les titres des publications.

Il a été décidé de séparer les titres anglais de ceux français pour pouvoir comparer les 2. En excluant les mots “Alzheimer”, “maladie” et “disease”, ces deux graphiques nous montrent les termes les plus fréquents.

Pour obtenir ce résultat lisible et interprétable, un nettoyage était nécessaire pour enlever les nombres, la ponctuation, les stopwords (les “mots vides” comme les articles, conjonctions, etc...). On remarque qu’il reste toutefois certains mots qui ne sont pas toujours pertinents : “quand”, “chez”.

Ce nettoyage des mots est assez fastidieux et demande une intervention en partie manuelle.

Dans le nuage de mot français, on remarque que le terme “mild” est toujours présent. Pendant nos analyses, on avait remarqué que les anglicismes dans les titres français n’étaient pas détectés et apparaissent donc dans notre nuage de mots français. Il est pertinent de les garder puisqu’ils sont utilisés en français.

D’autres mots anglais pouvaient apparaître malgré le filtrage de la langue, dû au fait que certains auteurs mettent leur titre en français et en anglais.

Dans ce cas, il est nécessaire d’avoir recourt à un nettoyage manuel, parfois au cas par cas.

En comparant ces deux graphiques, on peut voir que du côté francophone, le terme “personnes” est au même niveau que “cognitive” et “memory”. Ceci peut montrer que la recherche francophone s’intéresse avant tout à l’individu ou aux personnes aidant les malades. Tandis que la recherche anglaise explore les phénomènes relatifs au cerveau.

Les termes équivalents en français : “cognitive” et “mémoire” n’apparaissent qu’en 4ème position sur le nuage de mots. Avant cela, les mots “troubles” et “aidants” sont davantage récurrents. Puis, en rose, des mots comme “diagnostic”, “prise”, “clinique”, “éthique” laissent penser que la recherche francophone se concentre davantage sur l’accompagnement et la gestion des personnes atteintes de la maladie d’Alzheimer. Du côté anglophone, les termes qui ressortent sont relatifs à l’aspect concret de la maladie, ce qui se passe de manière technique et scientifique chez les personnes concernées par cette maladie.

### 7. Les institutions les plus actives dans la publications des articles des revues SHS

Pour identifier les institutions les plus actives dans la publication d'articles dans le domaine des sciences humaines et sociales (SHS) concernant la maladie d'Alzheimer, nous allons analyser les données de nos revues.

Nous allons supprimer les valeurs manquantes (NA) dans la colonne **`journalPublisher_s`** à cette étape plutôt que lors du nettoyage ultérieur. En procédant ainsi, nous nous assurons de conserver les lignes nécessaires à d'autres analyses tout en éliminant les données manquantes spécifiquement dans le contexte de la colonne des éditeurs (**`journalPublisher_s`**)

```{r, include=TRUE}
revues_shs_alzheimer <- revues_shs_alzheimer %>%
  filter(!is.na(journalPublisher_s))
```

Cette ligne de code utilise la fonction **`filter`** pour exclure les lignes où la colonne **`journalPublisher_s`** a des valeurs **`NA`**. Cela nettoie le dataframe en retirant les observations avec des valeurs manquantes dans la colonne spécifiée.

Nous allons maintenant sélectionner le top 10 des institutions de revue les plus actifs:

```{r, include=FALSE}
top_institutions <- revues_shs_alzheimer %>%
  count(journalPublisher_s) %>%
  arrange(desc(n)) %>%
  head(10)
```

Cette séquence de commandes utilise **`count`** pour compter le nombre d'occurrences de chaque éditeur de revue, puis **`arrange`** pour les trier par ordre décroissant en fonction du nombre de publications (**`n`**). Enfin, **`head(10)`** est utilisé pour sélectionner les 10 éditeurs avec le nb le plus élevé.

Nous allons maintenant créer une visualisation :

```{r, include=TRUE}
ggplot(top_institutions, aes(x = n, y = fct_reorder(journalPublisher_s, n))) +
  geom_bar(stat = "identity", fill = "#E6AB02", color = "black") +
  labs(
    title = "Institutions les plus actives dans les revues SHS",
    x = "Nombre de publications",
    y = "Institution"
  ) +
  scale_x_continuous(breaks = seq(0, max(top_institutions$n), by = 7)) +
  scale_y_discrete(labels = function(x) str_wrap(x, width = 40))+
  theme_minimal()
```

L’institution la plus active sur le sujet montre deux fois plus de publications avec celle en deuxième position. Il s’agit d’une grande maison d’édition installée depuis plusieurs siècles. On peut imaginer que les financements sont plus conséquents que parmi d’autres institutions ce qui lui permet d’être très présente dans le domaine.

# Création d'un taggage via Zotero

# Création d'une bibliographie sur Zotero

# Conclusion
